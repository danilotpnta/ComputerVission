{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\nKUL H02A5a Computer Vision: Group Assignment 2\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r1, r2, r3, r4, r5</span>. (fill in your student numbers!)\n\nIn this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 1* and you start from this template notebook. The notebook you submit for grading is the last notebook you submit in the [Kaggle competition](https://www.kaggle.com/t/d11be6a431b84198bc85f54ae7e2563f) prior to the deadline on **Tuesday 24 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 2* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n\n---------------------------------------------------------------\nNOTES:\n* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n","metadata":{"_cell_guid":"b47b15de-64a5-4fa9-a688-23d3efa9a2f4","_uuid":"0cc385a7-98f6-4883-96eb-7b89c7c9aa1c","papermill":{"duration":0.016533,"end_time":"2022-04-12T14:48:23.471825","exception":false,"start_time":"2022-04-12T14:48:23.455292","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Overview\nThis assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n* Image classification (Sect. 2)\n* Semantic segmentation (Sect. 3)\n* Adversarial attacks (Sect. 4)\n\nIn the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook.","metadata":{"_cell_guid":"35358cfb-b13d-4277-8dd5-4e663c8cd775","_uuid":"3b40b846-d7da-46d8-b354-c6d5c5ded56e","papermill":{"duration":0.014397,"end_time":"2022-04-12T14:48:23.501501","exception":false,"start_time":"2022-04-12T14:48:23.487104","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1.1 Deep learning resources\nIf you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO).","metadata":{"papermill":{"duration":0.014263,"end_time":"2022-04-12T14:48:23.530341","exception":false,"start_time":"2022-04-12T14:48:23.516078","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\nfrom keras.layers import Conv2D,MaxPooling2D,UpSampling2D,Input,BatchNormalization,Activation,Flatten, Dropout\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport keras\nimport cv2\n\nprint(\"Load modules complete!\")","metadata":{"_cell_guid":"7ddf657a-b938-4a49-87dc-b0db9af9156d","_uuid":"c65ea4f1-cc90-408f-b8e0-7c7399ec7e21","papermill":{"duration":5.416492,"end_time":"2022-04-12T14:48:28.961510","exception":false,"start_time":"2022-04-12T14:48:23.545018","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-21T18:02:21.813888Z","iopub.execute_input":"2023-05-21T18:02:21.814257Z","iopub.status.idle":"2023-05-21T18:02:21.821221Z","shell.execute_reply.started":"2023-05-21T18:02:21.814226Z","shell.execute_reply":"2023-05-21T18:02:21.820244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 PASCAL VOC 2009\nFor this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes.","metadata":{"papermill":{"duration":0.014416,"end_time":"2022-04-12T14:48:28.990998","exception":false,"start_time":"2022-04-12T14:48:28.976582","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Loading the training data\ntrain_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/train_set.csv', index_col=\"Id\")\nlabels = train_df.columns\ntrain_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\ntrain_df[\"seg\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\nprint(\"The training set contains {} examples.\".format(len(train_df)))\n\n# Show some examples\n# fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n# for i, label in enumerate(labels):\n#     df = train_df.loc[train_df[label] == 1]\n#     axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n#     axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n#     axs[0, i].axis(\"off\")\n#     axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n#     axs[1, i].axis(\"off\")\n    \n# plt.show()\n\n# # The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n# train_df.head(1)","metadata":{"_cell_guid":"1ce67f49-6bf6-4e5c-b5e4-576e893616a9","_uuid":"3b1c5fbb-757f-4349-b224-e281c540e1ad","papermill":{"duration":21.336481,"end_time":"2022-04-12T14:48:50.342062","exception":false,"start_time":"2022-04-12T14:48:29.005581","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-21T17:47:14.507288Z","iopub.execute_input":"2023-05-21T17:47:14.508195Z","iopub.status.idle":"2023-05-21T17:47:24.396157Z","shell.execute_reply.started":"2023-05-21T17:47:14.508159Z","shell.execute_reply":"2023-05-21T17:47:24.395063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_palette(num_classes):\n    \"\"\"\n    Maps classes to colors in the style of PASCAL VOC.\n    Close values are mapped to far colors for segmentation visualization.\n    See http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit\n\n    Takes:\n        num_classes: the number of classes\n    Gives:\n        palette: the colormap as a k x 3 array of RGB colors\n    \"\"\"\n    palette = np.zeros((num_classes, 3), dtype=np.uint8)\n    for k in range(0, num_classes):\n        label = k\n        i = 0\n        while label:\n            palette[k, 0] |= (((label >> 0) & 1) << (7 - i))\n            palette[k, 1] |= (((label >> 1) & 1) << (7 - i))\n            palette[k, 2] |= (((label >> 2) & 1) << (7 - i))\n            label >>= 3\n            i += 1\n    return palette\n\ndef color_seg(seg, palette):\n    \"\"\"\n    Replace classes with their colors.\n\n    Takes:\n        seg: H x W segmentation image of class IDs\n    Gives:\n        H x W x 3 image of class colors\n    \"\"\"\n    return palette[seg.flat].reshape(seg.shape + (3,))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:48:11.091775Z","iopub.execute_input":"2023-05-21T15:48:11.092148Z","iopub.status.idle":"2023-05-21T15:48:11.102081Z","shell.execute_reply.started":"2023-05-21T15:48:11.092117Z","shell.execute_reply":"2023-05-21T15:48:11.101015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colormap = make_palette(21)\ndf_labels = [train_df.columns[:-2][i] for i in range(len(labels))]\nclasses = ['background'] + df_labels \ncolormap2class = np.zeros(256 ** 3)      # Shape:(16777216,)\nfor i,cm in enumerate(colormap):\n    colormap2class[(cm[0] * 256 + cm[1]) * 256 + cm[2]] = i # build index\n\n\"\"\"\nKey: index of color map i.e [128, 0, 0] = 1\nVal: the array describing color map i.e [128, 0, 0]\n\n\"\"\"\nid_colormap = {k:v for k,v in enumerate(colormap)}","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:48:20.365778Z","iopub.execute_input":"2023-05-21T15:48:20.366163Z","iopub.status.idle":"2023-05-21T15:48:20.375374Z","shell.execute_reply.started":"2023-05-21T15:48:20.366129Z","shell.execute_reply":"2023-05-21T15:48:20.374475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(color_seg(train_df['seg'][0], palette))","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:14:19.538974Z","iopub.execute_input":"2023-05-21T15:14:19.539340Z","iopub.status.idle":"2023-05-21T15:14:19.814434Z","shell.execute_reply.started":"2023-05-21T15:14:19.539312Z","shell.execute_reply":"2023-05-21T15:14:19.813365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nnp.set_printoptions(threshold=sys.maxsize)\nprint(train_df[\"seg\"][0].shape)\nprint(train_df[\"seg\"][0][2].shape)\n# print(train_df[\"seg\"][0])\n\n\nplt.imshow(train_df[\"seg\"][0], vmin=0, vmax=3)\n# train_df[\"seg\"][0]","metadata":{"execution":{"iopub.status.busy":"2023-05-21T13:30:23.724589Z","iopub.execute_input":"2023-05-21T13:30:23.725490Z","iopub.status.idle":"2023-05-21T13:30:24.020458Z","shell.execute_reply.started":"2023-05-21T13:30:23.725453Z","shell.execute_reply":"2023-05-21T13:30:24.019388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the test data\ntest_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/test_set.csv', index_col=\"Id\")\ntest_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\ntest_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\nprint(\"The test set contains {} examples.\".format(len(test_df)))\n\n# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\ntest_df.head(1)","metadata":{"papermill":{"duration":11.507733,"end_time":"2022-04-12T14:49:02.044233","exception":false,"start_time":"2022-04-12T14:48:50.536500","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-21T12:15:21.963977Z","iopub.execute_input":"2023-05-21T12:15:21.964405Z","iopub.status.idle":"2023-05-21T12:15:36.528730Z","shell.execute_reply.started":"2023-05-21T12:15:21.964362Z","shell.execute_reply":"2023-05-21T12:15:36.527788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Your Kaggle submission\nYour filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook.","metadata":{"papermill":{"duration":0.197841,"end_time":"2022-04-12T14:49:02.437252","exception":false,"start_time":"2022-04-12T14:49:02.239411","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def _rle_encode(img):\n    \"\"\"\n    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n\n    Parameters\n    ----------\n    img: np.ndarray - binary img array\n    \n    Returns\n    -------\n    rle: String - running length encoded version of img\n    \"\"\"\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    rle = ' '.join(str(x) for x in runs)\n    return rle\n\ndef generate_submission(df):\n    \"\"\"\n    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n    \n    Parameters\n    ----------\n    df: pd.DataFrame - filled dataframe that needs to be converted\n    \n    Returns\n    -------\n    submission_df: pd.DataFrame - df in submission format.\n    \"\"\"\n    df_dict = {\"Id\": [], \"Predicted\": []}\n    for idx, _ in df.iterrows():\n        df_dict[\"Id\"].append(f\"{idx}_classification\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n    \n    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n    submission_df.to_csv(\"submission.csv\")\n    return submission_df","metadata":{"papermill":{"duration":0.213344,"end_time":"2022-04-12T14:49:02.848597","exception":false,"start_time":"2022-04-12T14:49:02.635253","status":"completed"},"tags":[],"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Semantic segmentation\nThe goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe).","metadata":{"papermill":{"duration":0.19763,"end_time":"2022-04-12T14:49:07.536010","exception":false,"start_time":"2022-04-12T14:49:07.338380","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class RandomSegmentationModel:\n    \"\"\"\n    Random segmentation model: \n        - generates random label maps for the inputs based on the class distributions observed during training\n        - every pixel in an input can only have one label\n    \"\"\"\n    def fit(self, X, Y):\n        \"\"\"\n        Adjusts the class ratio variable to the one observed in Y. \n\n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n        Y: list of arrays - n x (height x width)\n\n        Returns\n        -------\n        self\n        \"\"\"\n        self.distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n        print(\"Setting class distribution to:\\nbackground: {}\\n{}\".format(self.distribution[0], \"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution[1:]))))\n        return self\n        \n    def predict(self, X):\n        \"\"\"\n        Predicts for each input a label map.\n        \n        Parameters\n        ----------\n        X: list of arrays - n x (height x width x 3)\n            \n        Returns\n        -------\n        Y_pred: list of arrays - n x (height x width)\n        \"\"\"\n        np.random.seed(0)\n        return [np.random.choice(np.arange(len(labels) + 1), size=X_.shape[:2], p=self.distribution) for X_ in X]\n    \n    def __call__(self, X):\n        return self.predict(X)\n    \nmodel = RandomSegmentationModel()\nmodel.fit(train_df[\"img\"], train_df[\"seg\"])\ntest_df.loc[:, \"seg\"] = model.predict(test_df[\"img\"])\ntest_df.head(1)","metadata":{"papermill":{"duration":11.823715,"end_time":"2022-04-12T14:49:19.557577","exception":false,"start_time":"2022-04-12T14:49:07.733862","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Transfer Learning on Semantic Segmentation \nAs seen in the performance of the trained model from scrath efficiency can be improved. For this matter, we will train a model using transfer learning.\nIn Semantic Segmentation several architectures are found, amount popular optiosn are  VGG-UNet, FastFCN, DeepLab among others. We will be using the first approach namely VGG16, for more details visit: https://keras.io/api/applications/#inception3","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1 Analysig the data","metadata":{}},{"cell_type":"code","source":"# The dimensions of each image are diff\nprint('Inspecting img and smg sizes:')\nfor i in range(4):\n    print(\"Size img:\",train_df[\"img\"][i].shape)\n    print(\"Size lab:\", train_df[\"seg\"][i].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show some examples of img and its corresponding smg\nimport warnings\nimport matplotlib\nwarnings.filterwarnings(\"ignore\", category=matplotlib.MatplotlibDeprecationWarning)\n\ndef plot_img_seg(original_img, real_label, predicted_label=None):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    col = 2\n    if predicted_label is not None:\n        col = 3\n    # First Img\n    plt.subplot(int(f\"1{col}1\"))\n    plt.imshow(original_img)\n    plt.title(\"Original Image\")\n    \n    # Second Img\n    plt.subplot(int(f\"1{col}2\"))\n    plt.imshow(real_label)\n    plt.title(\"Truth Label\")\n    \n    # Third Img\n    if predicted_label is not None:\n        plt.subplot(int(f\"1{col}3\"))\n        plt.imshow(predicted_label)\n        plt.title(\"Predicted Label\")\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:47:47.270836Z","iopub.execute_input":"2023-05-21T17:47:47.271219Z","iopub.status.idle":"2023-05-21T17:47:47.278375Z","shell.execute_reply.started":"2023-05-21T17:47:47.271191Z","shell.execute_reply":"2023-05-21T17:47:47.277434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\ngray = train_df[\"seg\"][0]\ncolor = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n\ndef get_labelled_seg(img):\n    \"\"\"\n     For segmentation task, read color image, transform it to 3-d one-hot encodeing class target image.\n    :param img_path: the image path\n    :return: target: (image_size,image_size,21)\n    \"\"\"\n    image_size = 224\n#     colored_img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n    colored_img = color_seg(img, palette)\n    plt.imshow(colored_img)\n    \n    resized_img = resize(colored_img, (image_size, image_size))\n    img_224 = np.array(resized_img).astype('int32')    \n    print(img_224.shape)\n    \n    idx = (img_224[:, :, 0] * 256 + img_224[:, :, 1]) * 256 + img_224[:, :, 2]\n    print('id shape',idx.shape)\n    img_mapped = np.array(colormap2class[idx], dtype='int64') # 2-d class map, class of every pixel\n    \n    return img_mapped\n\na = get_labelled_seg(train_df['seg'][0])\n# plt.imshow(a)\n# plt.imshow(train_df['seg'][0])\n# a","metadata":{"execution":{"iopub.status.busy":"2023-05-21T15:25:42.386264Z","iopub.execute_input":"2023-05-21T15:25:42.386614Z","iopub.status.idle":"2023-05-21T15:25:42.768730Z","shell.execute_reply.started":"2023-05-21T15:25:42.386587Z","shell.execute_reply":"2023-05-21T15:25:42.767781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Resize images for VGG16","metadata":{}},{"cell_type":"code","source":"from skimage.transform import resize\nfrom skimage import io\nfrom PIL import Image\n\ndef get_labelled_seg(images_seg):\n    image_size = 224\n    \"\"\"\n     For segmentation task, read color image, transform it to 3-d one-hot encodeing class target image.\n    :param img_path: the image path\n    :return: target: (image_size,image_size,21)\n    \"\"\"\n    \n    train_df_seg224 = np.array([resize(img, (image_size, image_size), Image.ANTIALIAS) for img in images_seg]).astype('int32')    \n    train_df_seg_labelled = []\n    for img in train_df_seg224:\n        print(img.shape)\n        idx = (img[:, :, 0] * 256 + img[:, :, 1]) * 256 + img[:, :, 2]\n        train_df_seg_labelled.append(np.array(colormap2class[idx], dtype='int64')) # 2-d class map, class of every pixel\n        \n    return label_map\n\nimage_size = 224\ntrain_df_img224 = np.array([resize(img, (image_size, image_size, 3)) for img in train_df[\"img\"]]).astype('float32')\ntrain_df_seg224 = np.array([resize(img, (image_size, image_size)) for img in train_df[\"seg\"]]).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:48:47.961081Z","iopub.execute_input":"2023-05-21T17:48:47.961710Z","iopub.status.idle":"2023-05-21T17:49:04.639426Z","shell.execute_reply.started":"2023-05-21T17:48:47.961663Z","shell.execute_reply":"2023-05-21T17:49:04.638432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing Resizing\nlim = 1\nfor img in train_df_seg224[0:lim]:\n    fig, ax = plt.subplots(figsize=(5, 5))\n    plt.imshow(img)\n    \nfor img in train_df[\"seg\"][0:lim]:\n    fig, ax = plt.subplots(figsize=(5, 5))\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T16:48:47.585828Z","iopub.execute_input":"2023-05-21T16:48:47.586767Z","iopub.status.idle":"2023-05-21T16:48:48.100785Z","shell.execute_reply.started":"2023-05-21T16:48:47.586726Z","shell.execute_reply":"2023-05-21T16:48:48.099958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 Implementing transfering Learning","metadata":{}},{"cell_type":"markdown","source":"We will be implementing the following architecture, U-net follows from this U shape. In the first part the encoder is used to capture the context of the image. In the second part, in the decoder, we will use it to enable the precise localization. AS it can eb seen from the picture below there is no Dense layers above meaning we can take any picture size. Common structure for VGG16 is 224 img size  \n\n![](https://raw.githubusercontent.com/danilotpnta/ComputerVission/main/img/U-net.png)\nImg source: https://youtu.be/azM57JuQpQI\n\nThe above network uses skip connections, which means we will be skipping some layers in the NN. This skipping helps in traverse the information faster in the NN. It also helps in the problem of vanishing gradient descent. Thus, our Unet network will also account for this.","metadata":{}},{"cell_type":"markdown","source":"#### Lets create UNet, and the model","metadata":{}},{"cell_type":"markdown","source":"We will load the model imagenet weights and set the layers not to be trainable. The VGG16 represents only the encoder par thus, we need to define the decoder block.","metadata":{}},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n# Based on tinyurl.com/2ohv3d8z\n\ndef upSampling(filter,input,concat,concat_encoder):    \n    x = UpSampling2D(size=(2,2),interpolation='bilinear')(input) \n    x = Conv2D(filters=filter,kernel_size=(2,2),activation='relu',padding='same',kernel_initializer='he_normal')(x) \n    up_conv = BatchNormalization()(x)\n    \n    if concat:\n        merged = tf.keras.layers.concatenate([concat_encoder,up_conv])\n    else:\n        merged = up_conv\n    conv1 = BatchNormalization()(Conv2D(filters=filter,kernel_size=(3,3),activation='relu',padding='same',\n                                        kernel_initializer='he_normal')(merged))\n    conv2 = BatchNormalization()(Conv2D(filters=filter, kernel_size=(3, 3), activation='relu', padding='same',\n                                        kernel_initializer='he_normal')(conv1))\n\n    return conv2\n\n\ndef build_unet():\n    # Create a VGG16 model, and remove the last layer. This will be replaced with the images classes we have. \n    vgg16 = VGG16(weights='imagenet',include_top = False,input_shape=(image_size,image_size,3))\n    \n    # Sets layers not trainable\n    for layer in vgg16.layers:\n        layer.trainable = False\n        \n    image_feature = vgg16.output\n\n    n_classes = len(labels) + 1 # +1 background\n    conv1 = Conv2D(filters=1024,kernel_size=(3,3),activation='relu',padding='same',kernel_initializer='he_normal')(image_feature)\n    conv2 = Conv2D(filters=1024, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n    decoder_input = Dropout(0.5)(conv2)\n    \n    s1 = vgg16.get_layer(\"block1_conv2\").output         ## (512 x 512)\n    s2 = vgg16.get_layer(\"block2_conv2\").output         ## (256 x 256)\n    s3 = vgg16.get_layer(\"block3_conv2\").output         ## (128 x 128)\n    s4 = vgg16.get_layer(\"block4_conv3\").output         ## (64 x 64)\n    b1 = vgg16.get_layer(\"block5_conv3\").output         ## (32 x 32)\n\n    d1 = upSampling(filter=512,input=decoder_input,concat_encoder=b1,concat=True)\n    d2 = upSampling(filter=256,input=d1, concat_encoder=s4,concat=True)\n    d3 = upSampling(filter=128,input=d2, concat_encoder=s3,concat=True)\n    d4 = upSampling(filter=64, input=d3, concat_encoder=s2,concat=True)\n    d5 = upSampling(filter=64, input=d4, concat_encoder=s1,concat=False)\n\n    outputs = Conv2D(filters=n_classes,kernel_size=(1,1),activation='relu',padding='same',\n                    kernel_initializer='he_normal',name='seg_output')(d5)\n\n    model = Model(inputs=vgg16.input, outputs=outputs)\n    return model\n\n\n#Creating model object \n# model1 = build_unet()\n# model1.summary()\n# plot_model(model1, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T18:04:35.911864Z","iopub.execute_input":"2023-05-21T18:04:35.912242Z","iopub.status.idle":"2023-05-21T18:04:35.927497Z","shell.execute_reply.started":"2023-05-21T18:04:35.912211Z","shell.execute_reply":"2023-05-21T18:04:35.926597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\nfrom keras.utils.vis_utils import plot_model\n# Based on: tinyurl.com/2en5hr8k\n \ndef conv_block(input, num_filters):\n    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n \n    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n \n    return x\n \ndef decoder_block(input, skip_features, num_filters):\n    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n    x = Concatenate()([x, skip_features])\n    x = conv_block(x, num_filters)\n    return x\n \ndef build_vgg16_unet():\n \n    \"\"\" Pre-trained VGG16 Model \"\"\"\n    vgg16 = VGG16(weights='imagenet',include_top = False,input_shape= (image_size, image_size, 3))\n    \n    \"\"\" Sets layers not trainable \"\"\"\n    for layer in vgg16.layers:\n        layer.trainable = False\n \n    \"\"\" Encoder \"\"\"\n    s1 = vgg16.get_layer(\"block1_conv2\").output         ## (512 x 512)\n    s2 = vgg16.get_layer(\"block2_conv2\").output         ## (256 x 256)\n    s3 = vgg16.get_layer(\"block3_conv3\").output         ## (128 x 128)\n    s4 = vgg16.get_layer(\"block4_conv3\").output         ## (64 x 64)\n \n    \"\"\" Bridge \"\"\"\n    b1 = vgg16.get_layer(\"block5_conv3\").output         ## (32 x 32)\n \n    \"\"\" Decoder \"\"\"\n    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n \n    \"\"\" Output \"\"\"\n    n_classes = len(labels) + 1 # +1 background\n    outputs = Conv2D(n_classes, 1, padding=\"same\", activation=\"relu\")(d4)\n \n    model = Model(vgg16.input, outputs, name=\"VGG16_U-Net\")\n    return model\n\n# model = build_vgg16_unet()\n# model.summary()\n# plot_model(model, show_shapes=True, show_layer_names=True)\n\n# Checks if all layers are set to not trainable \n# layers = [(layer, layer.name, layer.trainable) for layer in model.layers]\n# model_df = pd.DataFrame(layers, columns=[\"Layer Type\", \"Layer Name\", \"Layer Trainable\"])\n# display(model_df)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:48:02.286406Z","iopub.execute_input":"2023-05-21T17:48:02.287428Z","iopub.status.idle":"2023-05-21T17:48:02.304201Z","shell.execute_reply.started":"2023-05-21T17:48:02.287354Z","shell.execute_reply":"2023-05-21T17:48:02.303220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating the Los function for optimizer","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n# Based on: tinyurl.com/2zyh26mg\n\ndef dice_coef(y_true, y_pred):\n    smooth = 1\n    y_pred = K.reshape(y_pred, (-1, K.int_shape(y_pred)[-1]))\n    y_true = K.one_hot(tf.compat.v1.to_int32(K.flatten(y_true)), K.int_shape(y_pred)[-1])\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f*y_true_f) + K.sum(y_pred_f*y_pred_f) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:48:05.228149Z","iopub.execute_input":"2023-05-21T17:48:05.228557Z","iopub.status.idle":"2023-05-21T17:48:05.241159Z","shell.execute_reply.started":"2023-05-21T17:48:05.228525Z","shell.execute_reply":"2023-05-21T17:48:05.240161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting the data","metadata":{}},{"cell_type":"code","source":"np.random.seed(12)\nfrom sklearn.model_selection import train_test_split\nXtrain_seg, Xtest_seg, Ytrain_seg, Ytest_seg = train_test_split(train_df_img224,train_df_seg224,test_size=0.1, random_state=12)\nprint(\"X_training Images Size:\", Xtrain_seg.shape)\nprint(\"Y_training Labels Size:\", Ytrain_seg.shape)\nprint(\"X_test Images Size:\", Xtest_seg.shape)\nprint(\"Y_test Labels Size:\", Ytest_seg.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:49:17.750655Z","iopub.execute_input":"2023-05-21T17:49:17.751471Z","iopub.status.idle":"2023-05-21T17:49:17.970574Z","shell.execute_reply.started":"2023-05-21T17:49:17.751392Z","shell.execute_reply":"2023-05-21T17:49:17.969722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at split data\nlim = 1\nfor img in Xtest_seg[0:lim]:\n    fig, ax = plt.subplots(figsize=(5, 5))\n    plt.imshow(img)\n    \nfor img in Ytest_seg[0:lim]:\n    fig, ax = plt.subplots(figsize=(5, 5))\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T17:23:29.273648Z","iopub.execute_input":"2023-05-21T17:23:29.274022Z","iopub.status.idle":"2023-05-21T17:23:29.970372Z","shell.execute_reply.started":"2023-05-21T17:23:29.273990Z","shell.execute_reply":"2023-05-21T17:23:29.969369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_samples_count = Xtrain_seg.shape[0]\nlr = 0.0001\nepochs = 25\nbatch_size = 24 \nimage_size = 224\ninput_shape = (image_size, image_size, 3)\n\nmodel1 = build_unet()\nmodel1.compile(optimizer=keras.optimizers.Adam(learning_rate = lr),loss=dice_coef_loss, metrics=[dice_coef])\nmodel1_history = model1.fit(Xtrain_seg, Ytrain_seg, \n                                         batch_size =  batch_size, \n                                         epochs = epochs, \n                                         validation_split=(0.1), shuffle=True)\n\n# model2 = build_vgg16_unet()\n# model2.compile(optimizer=keras.optimizers.Adam(learning_rate = lr, decay = lr/epochs ),loss=dice_coef_loss, metrics=[dice_coef])\n# model2_history = model.fit(Xtrain_seg, Ytrain_seg, \n#                           batch_size =  batch_size, \n#                           epochs = epochs, \n#                           validation_split=(0.15), shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T18:04:42.455769Z","iopub.execute_input":"2023-05-21T18:04:42.456148Z","iopub.status.idle":"2023-05-21T18:14:11.141344Z","shell.execute_reply.started":"2023-05-21T18:04:42.456119Z","shell.execute_reply":"2023-05-21T18:14:11.140408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the model weights after training\nfrom tensorflow.keras.models import load_model\nmodel1.save('VGG16_model1.h5')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-05-21T18:26:53.615089Z","iopub.execute_input":"2023-05-21T18:26:53.615511Z","iopub.status.idle":"2023-05-21T18:26:54.449116Z","shell.execute_reply.started":"2023-05-21T18:26:53.615479Z","shell.execute_reply":"2023-05-21T18:26:54.448104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Performance of the model 1 \"\"\"\n# for key in ['loss', 'val_loss',  'dice_coef', 'val_dice_coef' ]:\n#     plt.plot(model1_history.history[key],label=key)\n# plt.legend(['Loss', 'Loss: Val ',  'Loss: Dice_coef', 'Loss: Val Dice_coef' ])\n# plt.title('Transfer Learning Model Results')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Error\")\n# plt.savefig('Loss_model_tf')\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![results](https://raw.githubusercontent.com/danilotpnta/ComputerVission/main/img/Loss_model_tf.png)","metadata":{}},{"cell_type":"markdown","source":"#### Predicting on the test set","metadata":{}},{"cell_type":"code","source":"# Xtrain_seg, Xtest_seg, Ytrain_seg, Ytest_seg = train_test_split(train_df_img224,train_df_seg224,test_size=0.2, random_state=12)\ny_pred_model1 = model1.predict(Xtest_seg)\n# y_pred_model = model2.predict(Xtest_seg)","metadata":{"execution":{"iopub.status.busy":"2023-05-21T18:15:15.016171Z","iopub.execute_input":"2023-05-21T18:15:15.016557Z","iopub.status.idle":"2023-05-21T18:15:56.571825Z","shell.execute_reply.started":"2023-05-21T18:15:15.016527Z","shell.execute_reply":"2023-05-21T18:15:56.570761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model \ny_pred_max = np.argmax(y_pred_model1, axis=3)\nprint(y_pred_max.shape)\n\n# for i in range(5):\n#     plot_img_seg(Xtest_seg[i], Ytest_seg[i], y_pred_max[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading model from link","metadata":{}},{"cell_type":"code","source":"!wget -O transfer_learning.h5 https://dl.dropboxusercontent.com/s/tm64mq9e0i44lhx/VGG16_model1.h5?dl=0","metadata":{"execution":{"iopub.status.busy":"2023-05-21T19:03:47.600417Z","iopub.execute_input":"2023-05-21T19:03:47.600876Z","iopub.status.idle":"2023-05-21T19:04:06.953590Z","shell.execute_reply.started":"2023-05-21T19:03:47.600840Z","shell.execute_reply":"2023-05-21T19:04:06.952318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_tf_model1 = build_unet()\ntesting_tf_model1.load_weights(\"transfer_learning.h5\")\n\ny_pred_tf_model1 = model1.predict(Xtest_seg)\ny_pred_tf_max = np.argmax(y_pred_tf_model1, axis=3)\nprint(y_pred_tf_max.shape)\n\n# for i in range(5):\n#     plot_img_seg(Xtest_seg[i], Ytest_seg[i], y_pred_tf_max[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submit to competition\nYou don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details.","metadata":{"papermill":{"duration":0.196901,"end_time":"2022-04-12T14:49:19.951018","exception":false,"start_time":"2022-04-12T14:49:19.754117","status":"completed"},"tags":[]}},{"cell_type":"code","source":"generate_submission(test_df)","metadata":{"papermill":{"duration":81.176133,"end_time":"2022-04-12T14:50:41.324425","exception":false,"start_time":"2022-04-12T14:49:20.148292","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Adversarial attack\nFor this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations.","metadata":{"papermill":{"duration":0.197466,"end_time":"2022-04-12T14:50:41.721228","exception":false,"start_time":"2022-04-12T14:50:41.523762","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 5. Discussion\nFinally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision.","metadata":{"papermill":{"duration":0.195695,"end_time":"2022-04-12T14:50:42.117581","exception":false,"start_time":"2022-04-12T14:50:41.921886","status":"completed"},"tags":[]}}]}