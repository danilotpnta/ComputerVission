{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n!pip install deepface\n\nimport io # Input/Output Module\nimport os # OS interfaces\nimport cv2 # OpenCV package\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport statistics\nimport warnings\n\nfrom urllib import request # module for opening HTTP requests\nfrom matplotlib import pyplot as plt # Plotting library\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport seaborn as sns\nimport statistics\nfrom sklearn.manifold import TSNE\nfrom scipy.spatial.distance import cdist\n\nimport sklearn\nfrom sklearn.decomposition import PCA\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom deepface import DeepFace\nfrom operator import itemgetter\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.230891,"end_time":"2021-03-08T07:57:06.335029","exception":false,"start_time":"2021-03-08T07:57:06.104138","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T17:59:25.229837Z","iopub.execute_input":"2023-04-12T17:59:25.231444Z","iopub.status.idle":"2023-04-12T17:59:35.793064Z","shell.execute_reply.started":"2023-04-12T17:59:25.231385Z","shell.execute_reply":"2023-04-12T17:59:35.791849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"width:100%; height:140px\">\n    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n</div>\n\n\nKUL H02A5a Computer Vision: Group Assignment 1\n---------------------------------------------------------------\nStudent numbers: <span style=\"color:red\">r0916799, r0927846, r0928210, r0928016, r0922060</span>.\n\nThe goal of this assignment is to explore more advanced techniques for constructing features that better describe objects of interest and to perform face recognition using these features. This assignment will be delivered in groups of 5 (either composed by you or randomly assigned by your TA's).\n\nIn this assignment you are a group of computer vision experts that have been invited to ECCV 2021 to do a tutorial about  \"Feature representations, then and now\". To prepare the tutorial you are asked to participate in a kaggle competition and to release a notebook that can be easily studied by the tutorial participants. Your target audience is: (master) students who want to get a first hands-on introduction to the techniques that you apply.\n\n---------------------------------------------------------------\nThis notebook is structured as follows:\n0. Data loading & Preprocessing\n1. Feature Representations\n2. Evaluation Metrics \n3. Classifiers\n4. Experiments\n5. Publishing best results\n6. Discussion\n\nMake sure that your notebook is **self-contained** and **fully documented**. Walk us through all steps of your code. Treat your notebook as a tutorial for students who need to get a first hands-on introduction to the techniques that you apply. Provide strong arguments for the design choices that you made and what insights you got from your experiments. Make use of the *Group assignment* forum/discussion board on Toledo if you have any questions.\n\nFill in your student numbers above and get to it! Good luck! \n    \n---------------------------------------------------------------\n# 0. Data loading & Preprocessing\n\n## 0.1. Loading data\nThe training set is many times smaller than the test set and this might strike you as odd, however, this is close to a real world scenario where your system might be put through daily use! In this session we will try to do the best we can with the data that we've got! ","metadata":{"papermill":{"duration":0.022868,"end_time":"2021-03-08T07:57:06.382109","exception":false,"start_time":"2021-03-08T07:57:06.359241","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n\ntrain = pd.read_csv(\n    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2023/train_set.csv', index_col = 0)\ntrain.index = train.index.rename('id')\n\ntest = pd.read_csv(\n    '/kaggle/input/kul-h02a5a-computer-vision-ga1-2023/test_set.csv', index_col = 0)\ntest.index = test.index.rename('id')\n\n# read the images as numpy arrays and store in \"img\" column\ntrain['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2023/train/train_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in train.iterrows()]\n\ntest['img'] = [cv2.cvtColor(np.load('/kaggle/input/kul-h02a5a-computer-vision-ga1-2023/test/test_{}.npy'.format(index), allow_pickle=False), cv2.COLOR_BGR2RGB) \n                for index, row in test.iterrows()]\n  \n\ntrain_size, test_size = len(train),len(test)\n\n\"The training set contains {} examples, the test set contains {} examples.\".format(train_size, test_size)","metadata":{"papermill":{"duration":37.543619,"end_time":"2021-03-08T07:57:43.9495","exception":false,"start_time":"2021-03-08T07:57:06.405881","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T10:56:53.065865Z","iopub.execute_input":"2023-04-12T10:56:53.066872Z","iopub.status.idle":"2023-04-12T10:57:17.458309Z","shell.execute_reply.started":"2023-04-12T10:56:53.066826Z","shell.execute_reply":"2023-04-12T10:57:17.457347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Note: this dataset is a subset of the* [*VGG face dataset*](https://www.robots.ox.ac.uk/~vgg/data/vgg_face/).\n\n## 0.2. A first look\nLet's have a look at the data columns and class distribution.","metadata":{"papermill":{"duration":0.023377,"end_time":"2021-03-08T07:57:43.997466","exception":false,"start_time":"2021-03-08T07:57:43.974089","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# The training set contains an identifier, name, image information and class label\ntrain.head(1)","metadata":{"papermill":{"duration":3.315629,"end_time":"2021-03-08T07:57:47.336913","exception":false,"start_time":"2021-03-08T07:57:44.021284","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-11T01:00:30.300443Z","iopub.execute_input":"2023-04-11T01:00:30.301081Z","iopub.status.idle":"2023-04-11T01:00:32.680399Z","shell.execute_reply.started":"2023-04-11T01:00:30.301041Z","shell.execute_reply":"2023-04-11T01:00:32.679278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The test set only contains an identifier and corresponding image information.\n\ntest.head(1)","metadata":{"papermill":{"duration":3.283501,"end_time":"2021-03-08T07:57:50.644778","exception":false,"start_time":"2021-03-08T07:57:47.361277","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-11T01:00:32.683458Z","iopub.execute_input":"2023-04-11T01:00:32.683840Z","iopub.status.idle":"2023-04-11T01:00:35.173653Z","shell.execute_reply.started":"2023-04-11T01:00:32.683803Z","shell.execute_reply":"2023-04-11T01:00:35.172541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The class distribution in the training set:\ntrain.groupby('name').agg({'img':'count', 'class': 'max'})","metadata":{"papermill":{"duration":0.046628,"end_time":"2021-03-08T07:57:50.716317","exception":false,"start_time":"2021-03-08T07:57:50.669689","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-11T01:00:35.175457Z","iopub.execute_input":"2023-04-11T01:00:35.175842Z","iopub.status.idle":"2023-04-11T01:00:35.192216Z","shell.execute_reply.started":"2023-04-11T01:00:35.175806Z","shell.execute_reply":"2023-04-11T01:00:35.190943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that **Jesse is assigned the classification label 1**, and **Mila is assigned the classification label 2**. The dataset also contains 20 images of **look alikes (assigned classification label 0)** and the raw images. ","metadata":{}},{"cell_type":"markdown","source":"## 0.3. Preprocess data with DeepFacePreprocessor\n\nWe are using [DeepFace library](https://github.com/serengil/deepface) to detect and extract faces from the pictures. We can use different models for this task. We decided to use ssd model for it's speed and in cases it does not recognize a face, we are using better but slower model - retinaface.\n\nIn some cases there are multiple faces present on one picture, sometimes from 2 different classes. For training data we manually select the correct one, in other cases we take the first detected face.","metadata":{"papermill":{"duration":0.025108,"end_time":"2021-03-08T07:57:50.766719","exception":false,"start_time":"2021-03-08T07:57:50.741611","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DeepFacePreprocessor():\n    \"\"\"Preprocessing pipeline built around DeepFace face detector. \"\"\"\n    \n    def __init__(self, backend, face_size):\n        self.face_size = face_size\n        self.backend = backend\n            \n    def detect_faces(self, img, backend):\n        \"\"\"Detect all faces in an image.\"\"\"\n        return DeepFace.extract_faces(img, detector_backend = backend, enforce_detection=False, align=False, target_size=self.face_size)\n        \n    def extract_faces(self, img):\n        \"\"\"Returns 3 most probable faces (cropped) in an image.\"\"\"\n        \n        faces = self.detect_faces(img, self.backend)\n        faces.sort(key=itemgetter('confidence'), reverse=True)\n        # If no faces found, try again with a better backend\n        if faces[0]['confidence'] == 0:\n            faces = self.detect_faces(img, \"retinaface\")\n\n        # If still no faces found, return empty list\n        if faces[0]['confidence'] == 0:\n            return []\n        \n        #select 3 best matches\n        faces = faces[:3]\n        #normalize faces\n        faces_normalized = [cv2.normalize(face['face'], None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8) for face in faces]\n        return [cv2.cvtColor(face, cv2.COLOR_BGR2RGB) for face in faces_normalized]\n    \n    def preprocess(self, data_row, i, manual_choices=None):\n        faces = self.extract_faces(data_row['img'])\n        \n        # if no faces were found, return None\n        if len(faces) == 0:\n            nan_img = np.empty(self.face_size + (3,))\n            nan_img[:] = np.nan\n            return nan_img\n        elif len(faces) != 1 and manual_choices is not None and i in manual_choices:\n            #if we detect more than 1 face we can select correct one manually\n            return faces[manual_choices[i]]\n        \n        # if we didn't specify a face to choose manualy return the first one\n        return faces[0]\n        \n    def __call__(self, data, manual_choices=None):\n        return np.stack([self.preprocess(row, i, manual_choices) for i, row in data.iterrows()]).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T17:57:12.690933Z","iopub.execute_input":"2023-04-11T17:57:12.692052Z","iopub.status.idle":"2023-04-11T17:57:12.710522Z","shell.execute_reply.started":"2023-04-11T17:57:12.691983Z","shell.execute_reply":"2023-04-11T17:57:12.708801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you already have pre-processed images, you can uncomment the code below and use them.","metadata":{}},{"cell_type":"code","source":"# prep_path = '/kaggle/input/cv-ga1-dataset/'\n# train_X = np.load(os.path.join(prep_path, 'train_X.npy'))\n# train_y = np.load(os.path.join(prep_path, 'train_y.npy'))\n# test_X = np.load(os.path.join(prep_path, 'test_X.npy'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if preprocess:  \n    \n    # parameter to play with \n    FACE_SIZE = (150, 150)\n\n    def plot_image_sequence(data, n, imgs_per_row=7):\n        n_rows = 1 + int(n/(imgs_per_row+1))\n        n_cols = min(imgs_per_row, n)\n\n        f,ax = plt.subplots(n_rows,n_cols, figsize=(10*n_cols,10*n_rows))\n        for i in range(n):\n            if n == 1:\n                ax.imshow(data[i])\n            elif n_rows > 1:\n                ax[int(i/imgs_per_row),int(i%imgs_per_row)].imshow(data[i])\n            else:\n                ax[int(i%n)].imshow(data[i])\n        plt.show()\n\n    # we are using DeepFacePreprocessor since it has a better performance\n    # if data hasn't been preprocessed yet\n    prep_path = '/kaggle/working/prepped_data/'\n    if not os.path.exists(prep_path):\n        preprocessor = DeepFacePreprocessor(\"ssd\", FACE_SIZE)\n\n        manual_choices = {0: 0, 9: 0, 18: 1, 26: 0, 30: 0, 32: 0, 34: 2, 41: 0, 49:1, 50:0, 52:1, 53:1, 57:0, 59:1, 61:0, 70:0, 73:0, 77:0}\n        train_X, train_y = preprocessor(train, manual_choices), train['class'].values\n        test_X = preprocessor(test)\n\n        # Filtering imgs that are not faces ONCE\n        a = train_y[train_y == 2]\n        np.put(a, [24], 0)\n        train_y[train_y == 2] = a\n","metadata":{"papermill":{"duration":62.263517,"end_time":"2021-03-08T07:58:53.174859","exception":false,"start_time":"2021-03-08T07:57:50.911342","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T18:06:40.504451Z","iopub.execute_input":"2023-04-12T18:06:40.505580Z","iopub.status.idle":"2023-04-12T18:06:40.556674Z","shell.execute_reply.started":"2023-04-12T18:06:40.505523Z","shell.execute_reply":"2023-04-12T18:06:40.555338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # plot faces of Michael and Sarah\n\nplot_image_sequence(train_X[train_y == 0], n=10, imgs_per_row=10)","metadata":{"papermill":{"duration":2.635787,"end_time":"2021-03-08T07:58:55.836611","exception":false,"start_time":"2021-03-08T07:58:53.200824","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # plot faces of Jesse\n\n\nplot_image_sequence(train_X[train_y == 1], n=20, imgs_per_row=10)","metadata":{"papermill":{"duration":3.840961,"end_time":"2021-03-08T07:58:59.72249","exception":false,"start_time":"2021-03-08T07:58:55.881529","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # plot faces of Mila\n\nplot_image_sequence(train_X[train_y == 2], n=20, imgs_per_row=10)","metadata":{"papermill":{"duration":3.910256,"end_time":"2021-03-08T07:59:03.703299","exception":false,"start_time":"2021-03-08T07:58:59.793043","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code below saves the pre-processed data onto the current session. ","metadata":{"papermill":{"duration":0.100101,"end_time":"2021-03-08T07:59:04.315571","exception":false,"start_time":"2021-03-08T07:59:04.21547","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# # save preprocessed data\n# prep_path = '/kaggle/working/prepped_data/'\n# if not os.path.exists(prep_path):\n#     os.mkdir(prep_path)\n    \n# np.save(os.path.join(prep_path, 'train_X.npy'), train_X)\n# np.save(os.path.join(prep_path, 'train_y.npy'), train_y)\n# np.save(os.path.join(prep_path, 'test_X.npy'), test_X)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T18:27:04.644637Z","iopub.execute_input":"2023-04-11T18:27:04.645197Z","iopub.status.idle":"2023-04-11T18:27:05.697731Z","shell.execute_reply.started":"2023-04-11T18:27:04.645155Z","shell.execute_reply":"2023-04-11T18:27:05.696389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Feature Representations\n## 1.0. Example: Identify feature extractor\nOur example feature extractor doesn't actually do anything... It just returns the input:\n$$\n\\forall x : f(x) = x.\n$$\n\nIt does make for a good placeholder and baseclass ;).","metadata":{"papermill":{"duration":0.100212,"end_time":"2021-03-08T07:59:04.516059","exception":false,"start_time":"2021-03-08T07:59:04.415847","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class IdentityFeatureExtractor:\n    \"\"\"A simple function that returns the input\"\"\"\n    \n    def transform(self, X):\n        return X\n    \n    def __call__(self, X):\n        return self.transform(X)","metadata":{"papermill":{"duration":0.108781,"end_time":"2021-03-08T07:59:04.725071","exception":false,"start_time":"2021-03-08T07:59:04.61629","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T10:58:13.179744Z","iopub.execute_input":"2023-04-12T10:58:13.180143Z","iopub.status.idle":"2023-04-12T10:58:13.186847Z","shell.execute_reply.started":"2023-04-12T10:58:13.180109Z","shell.execute_reply":"2023-04-12T10:58:13.185531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.1. Scale Invariant Feature Transform\nSIFT is an algorithm that extracts keypoints and computes its descriptors. It is scale and rotation invariant which means we don't have to do any extra preprocessing as it would be needed for HOG which is not rotation invariant.","metadata":{"papermill":{"duration":0.134288,"end_time":"2021-03-08T07:59:04.959911","exception":false,"start_time":"2021-03-08T07:59:04.825623","status":"completed"},"tags":[]}},{"cell_type":"code","source":"warnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n        \nclass SIFTFeatureExtractor(IdentityFeatureExtractor):\n    \n    def __init__(self, **params):\n        self.params = params\n        self.sift = cv2.SIFT_create(\n                            nfeatures =  self.params.get('nfeatures'),\n                            nOctaveLayers = self.params.get('nOctaveLayers'),\n                            contrastThreshold = self.params.get('contrastThreshold'),\n                            edgeThreshold = self.params.get('edgeThreshold'),\n                            sigma = self.params.get('sigma') )\n        \n    def transform(self, X):        \n        images_descriptors = []\n        for img in X:\n            gray = cv2.cvtColor(img.astype(dtype=np.uint8), cv2.COLOR_BGR2GRAY)\n            _, descriptors = self.sift.detectAndCompute(gray, None)\n            images_descriptors.append(descriptors)\n        return np.asarray(images_descriptors)\n    \n    def flatten(self,image_descriptors):\n        # Filter Nones and flattens array to kx128 dimension\n        all_descriptors = []\n        for descriptors_per_img in image_descriptors:\n            if descriptors_per_img is not None:\n                for e in descriptors_per_img:\n                    all_descriptors.append(e)\n        return all_descriptors\n    \n    def compare_two_img(self, img1, img2):\n        img1 = img1.astype(dtype=np.uint8)\n        img2 = img2.astype(dtype=np.uint8)\n        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck = True)\n        \n        keypoints1, descriptors1 = self.sift.detectAndCompute(gray1, None)\n        keypoints2, descriptors2 = self.sift.detectAndCompute(gray2, None) \n        \n        if descriptors1 is None: \n            return print('Cannot proceed. Img1 has descriptors: None')\n        if descriptors2 is None:\n            return print('Cannot proceed. Img2 has descriptors: None')\n\n        matches = bf.match(descriptors1,descriptors2)\n        matches = sorted(matches, key = lambda x:x.distance)\n\n        img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches, outImg = np.empty((1,1)))\n        plt.imshow(img_matches)\n    \n    \n    def __call__(self, X):\n        return self.transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:02.553697Z","iopub.execute_input":"2023-04-12T17:43:02.554347Z","iopub.status.idle":"2023-04-12T17:43:02.571019Z","shell.execute_reply.started":"2023-04-12T17:43:02.554300Z","shell.execute_reply":"2023-04-12T17:43:02.569345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a SIFT extractor with choosen Hyperameters\n# sift_extractor = SIFTFeatureExtractor() \nsift_extractor = SIFTFeatureExtractor(\n                            nOctaveLayers = 10,\n                            contrastThreshold = 0.07,\n                            edgeThreshold = 10,\n                            sigma = 1.6)\n\n# Plot the matches between two faces\nsift_extractor.compare_two_img(train_X[12], train_X[17])","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:15:34.211208Z","iopub.execute_input":"2023-04-12T15:15:34.211896Z","iopub.status.idle":"2023-04-12T15:15:34.525277Z","shell.execute_reply.started":"2023-04-12T15:15:34.211859Z","shell.execute_reply":"2023-04-12T15:15:34.524230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.1. t-SNE Plots\nt-distributed stochastic neighbor embedding (t-SNE) is a method of visualising high-dimentional data by embedding it into low-dimentional space - usually two- or three-dimentional.","metadata":{"papermill":{"duration":0.100377,"end_time":"2021-03-08T07:59:05.372401","exception":false,"start_time":"2021-03-08T07:59:05.272024","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Defining TSNE with same seed for reproducibility\nseed = 42\ntsne = TSNE(random_state = seed, perplexity=15) \n\n# Colours for scatterplot \npalette = sns.color_palette(\"bright\", 3)\n\n# Function for creating a matrix with similarity-based distances between each image\ndef get_distance_matrix(images_descriptors):\n    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck = True)\n    features = []\n    for (i, descriptors1) in enumerate(images_descriptors):\n        features.append([])\n        for (j, descriptors2) in enumerate(images_descriptors):\n            if i == j:\n                distance = 0\n            elif descriptors1 is None or descriptors2 is None:\n                distance = 999\n            else:\n                matches = bf.match(descriptors1, descriptors2)\n                distance = statistics.mean([match.distance for match in matches])\n\n            features[i].append(distance)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:06.688256Z","iopub.execute_input":"2023-04-12T17:43:06.688968Z","iopub.status.idle":"2023-04-12T17:43:06.696951Z","shell.execute_reply.started":"2023-04-12T17:43:06.688930Z","shell.execute_reply":"2023-04-12T17:43:06.695848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defines SIFT extractor with specific hyperparameters\nsift_extractor = SIFTFeatureExtractor(\n                            nfeatures= None,\n                            nOctaveLayers = 4,\n                            contrastThreshold = 0.03,\n                            edgeThreshold = 20,\n                            sigma = 1.8)\n\n# Extracting the features\nall_features = sift_extractor.transform(train_X)\nall_features = get_distance_matrix(all_features)\n\n# Calculates pairs of instances in the new dimensional space\nfeatures_low_dimension = tsne.fit_transform(all_features)\ndf = pd.DataFrame(features_low_dimension, columns=[\"x\",\"y\"])\n\n# Create a Scatter plot\nsns.scatterplot(df, x=\"x\", y=\"y\", hue=train_y, legend='full', palette=palette)","metadata":{"papermill":{"duration":0.100308,"end_time":"2021-03-08T07:59:05.57403","exception":false,"start_time":"2021-03-08T07:59:05.473722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T17:43:08.928192Z","iopub.execute_input":"2023-04-12T17:43:08.929130Z","iopub.status.idle":"2023-04-12T17:43:16.776215Z","shell.execute_reply.started":"2023-04-12T17:43:08.929062Z","shell.execute_reply":"2023-04-12T17:43:16.775126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.1.2. Discussion\nIn this section we look how different SIFT hyperparameters affect feature extraction. We show the results on a t-SNE plot.\n\nLater we use the extracted features to train a model.","metadata":{"papermill":{"duration":0.100596,"end_time":"2021-03-08T07:59:05.775686","exception":false,"start_time":"2021-03-08T07:59:05.67509","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Looking at the effect of SIFT( nOctaveLayers: 3 Default)","metadata":{}},{"cell_type":"code","source":"acc_TSNE = []\nparameter = [3, 2, 4, 5] \n\nfor i in parameter:\n    sift_extractor = SIFTFeatureExtractor(\n                            nfeatures = None,\n                            nOctaveLayers = i,\n                            contrastThreshold = None,\n                            edgeThreshold = None,\n                            sigma = None)\n    \n    all_features = sift_extractor(train_X)\n    all_features = get_distance_matrix(all_features)\n    features_low_dimension = tsne.fit_transform(all_features)\n    df = pd.DataFrame(features_low_dimension, columns=[\"x\",\"y\"])\n    acc_TSNE.append(df)\n\n# Plotting\nf, ax = plt.subplots(1, len(parameter), figsize=(20,4))\nf.suptitle('Effect of varying SIFT-Hyperparameter: nOctaveLayers')\n\nfor i, df in enumerate(acc_TSNE):\n    sns.scatterplot(ax=ax[i], data=df, x=\"x\", y=\"y\", hue=train_y, legend='full', palette=palette)\n    ax[i].set_title(f\"Parameter: {parameter[i]}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:43:39.586217Z","iopub.execute_input":"2023-04-12T17:43:39.587230Z","iopub.status.idle":"2023-04-12T17:44:01.934483Z","shell.execute_reply.started":"2023-04-12T17:43:39.587193Z","shell.execute_reply":"2023-04-12T17:44:01.933282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looking at the effect of SIFT( contrastThreshold = 0.04)\ncontrastThreshold -> The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. \nThe larger the threshold, the less features are produced by the detector.","metadata":{}},{"cell_type":"code","source":"acc_TSNE = []\nparameter = [None, 0.04, 0.08, 0.05] \n\nfor i in parameter:\n    sift_extractor = SIFTFeatureExtractor(\n                            nfeatures = None,\n                            nOctaveLayers = None,\n                            contrastThreshold = i,\n                            edgeThreshold = None,\n                            sigma = None)\n    \n    all_features = sift_extractor(train_X)\n    all_features = get_distance_matrix(all_features)\n    features_low_dimension = tsne.fit_transform(all_features)\n    df = pd.DataFrame(features_low_dimension, columns=[\"x\",\"y\"])\n    acc_TSNE.append(df)\n\n# Plotting\nf, ax = plt.subplots(1, len(parameter), figsize=(20,4))\nf.suptitle('Effect of  SIFT-Hyperparameters')\n\nfor i, df in enumerate(acc_TSNE):\n    sns.scatterplot(ax=ax[i], data=df, x=\"x\", y=\"y\", hue=train_y, legend='full', palette=palette)\n    ax[i].set_title(f\"Parameter: {parameter[i]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looking at the effect of SIFT( edgeThreshold = 10)\nedgeThreshold -> The threshold used to filter out edge-like features.\nThe larger the edgeThreshold, the less features are filtered out (more features are retained).","metadata":{}},{"cell_type":"code","source":"acc_TSNE = []\nparameter = [10, 15, 20, 8] \n\nfor i in parameter:\n    sift_extractor = SIFTFeatureExtractor(\n                            nfeatures = None,\n                            nOctaveLayers = None,\n                            contrastThreshold = None,\n                            edgeThreshold = i,\n                            sigma = None)\n    \n    all_features = sift_extractor(train_X)\n    all_features = get_distance_matrix(all_features)\n    features_low_dimension = tsne.fit_transform(all_features)\n    df = pd.DataFrame(features_low_dimension, columns=[\"x\",\"y\"])\n    acc_TSNE.append(df)\n\n# Plotting\nf, ax = plt.subplots(1, len(parameter), figsize=(20,4))\nf.suptitle('Effect of  SIFT-Hyperparameters')\n\nfor i, df in enumerate(acc_TSNE):\n    sns.scatterplot(ax=ax[i], data=df, x=\"x\", y=\"y\", hue=train_y, legend='full', palette=palette)\n    ax[i].set_title(f\"Parameter: {parameter[i]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looking at the effect of SIFT( sigma = 1.6)\nsigma -> The sigma of the Gaussian applied to the input image at the octave #0. If your image is captured with a weak camera with soft \nlenses, you might want to reduce the number.","metadata":{}},{"cell_type":"code","source":"acc_TSNE = []\nparameter = [None, 1.6, 1.3, 1.8] \n\nfor i in parameter:\n    sift_extractor = SIFTFeatureExtractor(\n                            nfeatures = None,\n                            nOctaveLayers = None,\n                            contrastThreshold = None,\n                            edgeThreshold = None,\n                            sigma = i)\n    \n    all_features = sift_extractor(train_X)\n    all_features = get_distance_matrix(all_features)\n    features_low_dimension = tsne.fit_transform(all_features)\n    df = pd.DataFrame(features_low_dimension, columns=[\"x\",\"y\"])\n    acc_TSNE.append(df)\n\n# Plotting\nf, ax = plt.subplots(1, len(parameter), figsize=(20,4))\nf.suptitle('Effect of  SIFT-Hyperparameters')\n\nfor i, df in enumerate(acc_TSNE):\n    sns.scatterplot(ax=ax[i], data=df, x=\"x\", y=\"y\", hue=train_y, legend='full', palette=palette)\n    ax[i].set_title(f\"Parameter: {parameter[i]}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Baseline 2: PCA feature extractor\nBelow our implementation of PCA can be observed.","metadata":{"papermill":{"duration":0.101426,"end_time":"2021-03-08T07:59:05.978236","exception":false,"start_time":"2021-03-08T07:59:05.87681","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class PCAFeatureExtractor(IdentityFeatureExtractor):\n    \n    def __init__(self, n_components, data):\n        self.n_components = n_components      \n        self.data = self.preprocess_data(data)\n        self.mean = np.mean(self.data, axis=0)\n        self.singular, self.eigenfaces = self.compute_vectors()\n        \n    def svd_flip(self, u, v, u_based_decision=True): \n        \"\"\"Source : Scikit-Learn documentation\"\"\"\n        if u_based_decision:\n            # columns of u, rows of v\n            max_abs_cols = np.argmax(np.abs(u), axis=0)\n            signs = np.sign(u[max_abs_cols, range(u.shape[1])])\n            u *= signs\n            v *= signs[:, np.newaxis]\n        else:\n            # rows of v, columns of u\n            max_abs_rows = np.argmax(np.abs(v), axis=1)\n            signs = np.sign(v[range(v.shape[0]), max_abs_rows])\n            u *= signs\n            v *= signs[:, np.newaxis]\n        return u, v\n    \n    def compute_vectors(self):\n        \"\"\"Compute the eigenvectors and the corrsiponding singular values\"\"\"\n        data = self.data - self.mean\n        U, S, Vt = np.linalg.svd(data, full_matrices=False)\n        U, Vt = self.svd_flip(U, Vt)\n        eig_vecs = Vt[:self.n_components]\n        #return the singular values and first n eig_vectors\n        return S, eig_vecs\n    \n    def transform(self, X):\n        \"\"\"Transform data into set of features\"\"\"\n        X_gray = self.preprocess_data(X)\n        new_X = X_gray - self.mean\n        return np.dot(new_X, self.eigenfaces.T)\n    \n    def inverse_transform(self, X):\n        \"\"\"transform set of features into data\"\"\"\n        return np.dot(X, self.eigenfaces) + self.mean\n    \n    \n    def preprocess_data(self, X):\n        \"\"\"Convert a list of images to gray scale and then build a matrix with them\"\"\"\n        facematrix = []\n        if X.ndim == 4:\n            X_gray = np.zeros(X.shape[:-1])\n            for i in range(X.shape[0]): \n                X_gray[i] = cv2.cvtColor((X[i]).astype(np.uint8), cv2.COLOR_BGR2GRAY) \n                facematrix.append(X_gray[i].flatten())\n\n            facematrix = np.array(facematrix)\n        else:\n            X_gray = cv2.cvtColor((X).astype(np.uint8), cv2.COLOR_BGR2GRAY) \n            facematrix = X_gray.flatten()\n        \n        return facematrix","metadata":{"papermill":{"duration":0.111032,"end_time":"2021-03-08T07:59:06.191215","exception":false,"start_time":"2021-03-08T07:59:06.080183","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T14:56:55.930955Z","iopub.execute_input":"2023-04-12T14:56:55.931936Z","iopub.status.idle":"2023-04-12T14:56:55.948281Z","shell.execute_reply.started":"2023-04-12T14:56:55.931895Z","shell.execute_reply":"2023-04-12T14:56:55.947193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using PCA for predictions\nWe can already use the pca to find the best match for an image in our dataset without building a model. To do that we extract the features array from the image and then from the whole dataset, finally we compare the arrays and get the imagein the databse with the smaller euclidean distance from our target image.","metadata":{}},{"cell_type":"code","source":"pca1 = PCAFeatureExtractor(5, train_X)\n#THIqueryS IS A TEST FOR THE PCA\nweights = pca1.transform(train_X)\n# Test on out-of-sample image of existing class\nquery = test_X[11]\n\nprint(query.shape)\nquery_weight = pca1.transform(query)\neuclidean_distance = np.linalg.norm(weights - query_weight, axis=1)\nbest_match = np.argmin(euclidean_distance)\nprint(\"Best match %s with Euclidean distance %f\" % (train_y[best_match], euclidean_distance[best_match]))\n# Visualize\nfig, axes = plt.subplots(1,2,sharex=True,sharey=True,figsize=(8,6))\naxes[0].imshow(query, cmap=\"gray\")\naxes[0].set_title(\"Query\")\naxes[1].imshow(train_X[best_match], cmap=\"gray\")\naxes[1].set_title(\"Best match\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T14:56:59.844091Z","iopub.execute_input":"2023-04-12T14:56:59.845302Z","iopub.status.idle":"2023-04-12T14:57:00.648674Z","shell.execute_reply.started":"2023-04-12T14:56:59.845233Z","shell.execute_reply":"2023-04-12T14:57:00.647669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choosing the right number of components\n\nThis could be done either by testing different values and choosing the one that leads to greater accuracy or by analyzing the singular values. Each singular value indicates how much information does the corresponding eigenvector contain. From the following plot we notice that after the 5th vector the ammount of information start to decrease less rapidly so a good number of components would probably be around 5. This could be a good trade-off between number of features and accuracy.","metadata":{}},{"cell_type":"code","source":"#plotting first 15 singular values\nplt.plot(range(15), pca1.singular[:15])","metadata":{"execution":{"iopub.status.busy":"2023-04-11T13:41:46.502938Z","iopub.execute_input":"2023-04-11T13:41:46.504297Z","iopub.status.idle":"2023-04-11T13:41:46.701409Z","shell.execute_reply.started":"2023-04-11T13:41:46.504251Z","shell.execute_reply":"2023-04-11T13:41:46.700026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.1. Eigenface Plots","metadata":{"papermill":{"duration":0.100881,"end_time":"2021-03-08T07:59:06.392861","exception":false,"start_time":"2021-03-08T07:59:06.29198","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,4,sharex=True,sharey=True,figsize=(8,10))\nfor i in range(4):\n    axes[i].imshow(pca1.eigenfaces[i].real.reshape((150,150)), cmap=\"gray\")\nplt.show()","metadata":{"papermill":{"duration":0.100638,"end_time":"2021-03-08T07:59:06.595002","exception":false,"start_time":"2021-03-08T07:59:06.494364","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-11T19:50:18.802783Z","iopub.execute_input":"2023-04-11T19:50:18.803854Z","iopub.status.idle":"2023-04-11T19:50:19.269787Z","shell.execute_reply.started":"2023-04-11T19:50:18.803791Z","shell.execute_reply":"2023-04-11T19:50:19.268183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now see how we are able to reconstruct a face using the features and a certain number of eigenfaces. We notice that the face start to be distinguishable already after a small number of eigenfaces, and after a certain number adding another eigenface does not add a lot to the picture. This is because the first egienfaces are the one that \"contain\" more information while the last ones encapsulate small features about the image. This is the reason why we can choose to only use a certain number of eigenfaces as features to make the model less complex but still pretty accurate.","metadata":{}},{"cell_type":"code","source":"values = [2, 5, 15, 30, 80]\nplotPCA = [PCAFeatureExtractor(i, train_X) for i in values]\nquery = test_X[8]\nfig, axes = plt.subplots(2,3,sharex=True,sharey=True,figsize=(9,6))\naxes[0][0].imshow(query, cmap=\"gray\")\naxes[0][0].set_title(\"Original face\")\nfeatures = [plotPCA[i].transform(query) for i in range(5)]\ninverse = [plotPCA[i].inverse_transform(features[i]) for i in range(5)]\naxes[0][1].imshow(inverse[0].reshape((150,150)), cmap=\"gray\")\naxes[0][1].set_title(\"using 2 eigenfaces\")\naxes[0][2].imshow(inverse[1].reshape((150,150)), cmap=\"gray\")\naxes[0][2].set_title(\"using 5 eigenfaces\")\naxes[1][0].imshow(inverse[2].reshape((150,150)), cmap=\"gray\")\naxes[1][0].set_title(\"using 15 eigenfaces\")\naxes[1][1].imshow(inverse[3].reshape((150,150)), cmap=\"gray\")\naxes[1][1].set_title(\"using 30 eigenfaces\")\naxes[1][2].imshow(inverse[4].reshape((150,150)), cmap=\"gray\")\naxes[1][2].set_title(\"using 80 eigenfaces\")","metadata":{"execution":{"iopub.status.busy":"2023-04-12T14:58:50.494701Z","iopub.execute_input":"2023-04-12T14:58:50.495479Z","iopub.status.idle":"2023-04-12T14:58:53.018275Z","shell.execute_reply.started":"2023-04-12T14:58:50.495438Z","shell.execute_reply":"2023-04-12T14:58:53.017223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.2. Feature Space Plots\n\nBelow the feature space plot can be observed. We can see that classes are clustered together: Jesse and Michael are close by, each near the top of the plot (Jesse clustered on the left and Michael on the right). Similarly, Sarah and Mila are also visibly in a different cluster, Mila is clustered on the bottom right while Sarah is clustered on the bottom left. This shows us that even with 2 principle components, which hold most of the varience, PCA can identify differences in test data distinctively.","metadata":{"papermill":{"duration":0.101263,"end_time":"2021-03-08T07:59:06.797448","exception":false,"start_time":"2021-03-08T07:59:06.696185","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from matplotlib.offsetbox import OffsetImage, AnnotationBbox\nplt.rcParams[\"figure.autolayout\"] = True\n\ndef getImage(path):\n    return OffsetImage(test_X[path], zoom=.2)\n\npcaPlot = PCAFeatureExtractor(2, train_X)\ncoord = pcaPlot.transform(test_X)\nfig, ax = plt.subplots(figsize=(12, 12))\nplt.xticks(range(-10000,10000,2000))\nplt.yticks(range(-6500,7000,2000))\nfor x0, y0, path in zip(coord[:,0], coord[:,1], [i for i in range(1816)]):\n    ab = AnnotationBbox(getImage(path), (x0, y0), frameon=False)\n    ax.add_artist(ab)\n    ax.set_xlabel(\"eigenface 1\")\n    ax.set_ylabel(\"eigenface 2\")\nplt.show()\n","metadata":{"papermill":{"duration":0.101801,"end_time":"2021-03-08T07:59:07.000598","exception":false,"start_time":"2021-03-08T07:59:06.898797","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T17:15:44.320110Z","iopub.execute_input":"2023-04-12T17:15:44.320513Z","iopub.status.idle":"2023-04-12T17:15:55.493833Z","shell.execute_reply.started":"2023-04-12T17:15:44.320481Z","shell.execute_reply":"2023-04-12T17:15:55.491766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2.3. Discussion\n\n#### PCA\nPCA (Principal Component Analysis) is a technique widely used in Machine Learning to reduce the dimensionality of a dataset while retaining as much information as possible. In order to do that it transforms the original dataset into a new coordinate system in which each dimension is a linear combination of the original variables. PCA requires the dataset to be centered (the mean has to be 0). After that we compute the covariance matrix of the data and use that to extract the eigenvalues and eigenvectors. Eigenvectors represents directions in which the data varies the most, in the particular case of face detection these vectors are also called eigenfaces. By projecting a data point onto these eigenvectors we get features which are used to train a model. The eigenvalues express how much variance is “contained” in each eigenvector. The eigenvectors with more variance are also the ones that contain the more information and, hence, the ones we should use to extract important features in face recognition. This is why we usually decide to only use the first n eigenvectors (choosing n is usually a trade off between complexity and accuracy of the model).\n#### SVD\nIn our case we found the computation of the eigenvectors to be really slow and inefficient so we decide to use SVD to reach an equivalent result. SVD (Singular Value Decomposition) is another technique used to reduce dimensionality of a dataset. This technique takes a matrix A and decompose it in three matrices U,S and V. S contains the singular values which can be seen as the eigenvalues and represent the importance of each principal component. U and V are the left and right singular vectors which can be used to transform the data into the new coordinate system (as the eigenvectors). When the matrix is centered the eigenvectors and the columns of U are equivalent.","metadata":{"papermill":{"duration":0.102099,"end_time":"2021-03-08T07:59:07.204783","exception":false,"start_time":"2021-03-08T07:59:07.102684","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 2. Evaluation Metrics\n## 2.0. Accuracy\nAs example metric we take the accuracy. Informally, accuracy is the proportion of correct predictions over the total amount of predictions. It is used a lot in classification but it certainly has its disadvantages...","metadata":{"papermill":{"duration":0.10088,"end_time":"2021-03-08T07:59:07.406787","exception":false,"start_time":"2021-03-08T07:59:07.305907","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"papermill":{"duration":1.180116,"end_time":"2021-03-08T07:59:08.688561","exception":false,"start_time":"2021-03-08T07:59:07.508445","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T11:11:06.719661Z","iopub.execute_input":"2023-04-12T11:11:06.720034Z","iopub.status.idle":"2023-04-12T11:11:06.725184Z","shell.execute_reply.started":"2023-04-12T11:11:06.720003Z","shell.execute_reply":"2023-04-12T11:11:06.724096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Classifiers\n\nNow that we have created two different feature extraction models, its time to put these to the test! Below, we will create ***NUMBER OF CLASSIFIERS*** models to classify the images. We have tried multiple different models, for each type of classifier, and these will be mentioned in their respective sections. ","metadata":{"papermill":{"duration":0.103749,"end_time":"2021-03-08T07:59:08.894358","exception":false,"start_time":"2021-03-08T07:59:08.790609","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 3.1 Support Vector Machine\n\nBelow two different SVM methods can be observed, one for SIFT and one for PCA.","metadata":{}},{"cell_type":"markdown","source":"### 3.1.1 SVM for SIFT\n\nWe defined two functions:\n\n- 'kmean_bow' is used to cluster all the features into small groups. For instance, an image will have certain features that will be grouped into similar groups. [[1]](#1)\n\n<img width=\"700\" align=\"center\" src=\"https://raw.githubusercontent.com/danilotpnta/ComputerVission/main/img/2.png\" >\n\n- 'create_features_bow' this is used to calculate the features that will be used to train our model. It calculates the minimum distance of the descriptors of one img with klusters created previously. The minimum distance then between clusters is selected to describe the features of an image.\n\n![Features of an iamge](https://raw.githubusercontent.com/danilotpnta/ComputerVission/main/img/4.png)\n\nIt will escentially look like the img above where the x-axis is the BoW and the frequencies are the histogram that describes uniquely one image. [[2]](#2).\n\n#### References\n<a id=\"1\">[1]</a> \nC 7.1 | Bag Of Visual Words | CNN | Object Detection | Machine learning | EvODN. Available at: https://youtu.be/1_5uuqWXuIA\n\n<a id=\"2\">[2]</a> \nBag of Visual Words Model for Image Classification and Recognition. Available at: : https://kushalvyas.github.io/BOV.html\n","metadata":{}},{"cell_type":"code","source":"# Creating a Bag of Word from the descriptors\ndef kmean_bow(all_descriptors, num_cluster):\n    bow_dict = []\n    kmeans = KMeans(n_clusters = num_cluster)\n    kmeans.fit(all_descriptors)\n    bow_dict = kmeans.cluster_centers_\n\n    return bow_dict\n\ndef create_kmean(all_descriptors, num_cluster):\n    kmeans = KMeans(n_clusters = num_cluster)\n    kmeans.fit(all_descriptors)\n    return kmeans\n    \n\n# Creates features from the BoW\ndef create_feature_bow(image_descriptors, BoW, num_cluster):\n\n    X_features = []\n\n    for i in range(len(image_descriptors)):\n        features = np.array([0] * num_cluster)\n\n        if image_descriptors[i] is not None:\n            '''\n            Compare per each image k*descriptors with the BoW\n            BoW shape: (30, 128)\n            One image k*descriptors: (168, 128)\n            '''\n            distance = cdist(image_descriptors[i], BoW, metric='euclidean')\n            \n            # Along 168 distances calculated, get the min index\n            argmin = np.argmin(distance, axis = 1)   \n            \n            for j in argmin:\n                features[j] += 1\n        X_features.append(features)\n        \n    return X_features","metadata":{"execution":{"iopub.status.busy":"2023-04-12T17:58:27.853159Z","iopub.execute_input":"2023-04-12T17:58:27.853904Z","iopub.status.idle":"2023-04-12T17:58:27.862622Z","shell.execute_reply.started":"2023-04-12T17:58:27.853863Z","shell.execute_reply":"2023-04-12T17:58:27.861507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Contains descriptors of 80 without filtering Nones\nimage_descriptors = sift_extractor(train_X)\n\n# Filter Nones and flattens array to kx128 dimension\nall_descriptors = sift_extractor.flatten(image_descriptors)\n\nnum_cluster = 70      \nBoW = kmean_bow(all_descriptors, num_cluster = num_cluster)\nkmeans = create_kmean(all_descriptors, num_cluster = num_cluster)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:00:00.571975Z","iopub.execute_input":"2023-04-12T18:00:00.572407Z","iopub.status.idle":"2023-04-12T18:00:13.861749Z","shell.execute_reply.started":"2023-04-12T18:00:00.572369Z","shell.execute_reply":"2023-04-12T18:00:13.860870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_features = create_feature_bow(image_descriptors, BoW, num_cluster)\nprint(np.array(X_features).shape)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_features, train_y, test_size = 0.1, random_state = 1)\nprint(\"X_train shape: \", np.array(X_train).shape)\nprint(\"y_train shape: \", np.array(Y_train).shape)\nprint(\"-------------------------------\")\nprint(\"X_test shape: \", np.array(X_test).shape)\nprint(\"y_test shape: \", np.array(Y_test).shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:00:20.893858Z","iopub.execute_input":"2023-04-12T18:00:20.894223Z","iopub.status.idle":"2023-04-12T18:00:20.953594Z","shell.execute_reply.started":"2023-04-12T18:00:20.894191Z","shell.execute_reply":"2023-04-12T18:00:20.952368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp = MLPClassifier(verbose=False, max_iter=6000)\nmlp.fit(X_train, Y_train)\n\n# clf = GridSearchCV(mlp2, parameter_space, n_jobs=-1, cv=3)\n# clf.fit(X_train, Y_train)\nmlp_better = MLPClassifier(verbose=False, max_iter=2000, activation='tanh', \n                           alpha=0.0001, hidden_layer_sizes=(50, 50, 50), \n                           learning_rate= 'constant', solver= 'adam')\nmlp_better.fit(X_train, Y_train)\n\n\nkNN = KNeighborsClassifier(n_neighbors = 4, p = 1)\nkNN.fit(X_train,Y_train)\n\nrndForest = RandomForestClassifier(n_estimators=75, random_state=1)\nrndForest.fit(X_train,Y_train)\n\nmodel_svm = SVC(random_state = 1, max_iter = 120)\nmodel_svm.fit(X_train, Y_train)\n\nensemble = VotingClassifier(estimators=[\n    ('mlp', mlp_better), \n    ('kNN', kNN), \n    ('model_svm', model_svm)], voting='hard')\nensemble.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:00:23.078676Z","iopub.execute_input":"2023-04-12T18:00:23.079674Z","iopub.status.idle":"2023-04-12T18:00:23.737362Z","shell.execute_reply.started":"2023-04-12T18:00:23.079619Z","shell.execute_reply":"2023-04-12T18:00:23.736347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\"mlp\" model: ')\nprint(\"score on training set params: \", mlp.score(X_train, Y_train))\nprint(\"score on testing set params: \", mlp.score(X_test, Y_test))\n\nprint('\\n\"mlp_better\" model: ')\nprint(\"score on training set params: \", mlp_better.score(X_train, Y_train))\nprint(\"score on testing set params: \", mlp_better.score(X_test, Y_test))\n\nprint('\\n\"model_svm\" model: ')\nprint(\"score on training set params: \", model_svm.score(X_train, Y_train))\nprint(\"score on testing set params: \", model_svm.score(X_test, Y_test))\n\nprint('\\n\"kNN\" model: ')\nprint(\"score on training set params: \", kNN.score(X_train, Y_train))\nprint(\"score on testing set params: \", kNN.score(X_test, Y_test))\n\nprint('\\n\"rndForest\" model: ')\nprint(\"score on training set params: \", rndForest.score(X_train, Y_train))\nprint(\"score on testing set params: \", rndForest.score(X_test, Y_test))\n\nprint('\\n\"model_svm\" model: ')\nprint(\"score on training set params: \", model_svm.score(X_train, Y_train))\nprint(\"score on testing set params: \", model_svm.score(X_test, Y_test))\n\nprint('\\n\"ensemble\" model: ')\nprint(\"score on training set params: \", ensemble.score(X_train, Y_train))\nprint(\"score on testing set params: \", ensemble.score(X_test, Y_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:00:26.402437Z","iopub.execute_input":"2023-04-12T18:00:26.403574Z","iopub.status.idle":"2023-04-12T18:00:26.455357Z","shell.execute_reply.started":"2023-04-12T18:00:26.403520Z","shell.execute_reply":"2023-04-12T18:00:26.454240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2 SVM for PCA\n\nWe use the radial basis function kernel for the classification. We have also tried other kernels; however, the best one seemed to be RBF. We have also combined some other SVM models together to create and ensemble model with soft voting. Although it works great on the training dataset, it is not general and hence we decided not to use it (but it is still implemented here for observation). ","metadata":{}},{"cell_type":"code","source":"class SVM:\n\n    def __init__(self, ensemble=False):\n\n        if not ensemble:\n            self.model = svm.SVC(probability=True, kernel=\"rbf\")\n        \n        else:\n            models = list()\n            models.append(('svm0', svm.SVC(probability=True, kernel=\"rbf\")))\n            models.append(('svm1', svm.SVC(probability=True, kernel='poly', degree=2)))\n            models.append(('svm2', svm.SVC(probability=True, kernel='poly', degree=3)))\n            models.append(('svm3', svm.SVC(probability=True, kernel='linear')))\n            models.append(('svm4', svm.SVC(probability=True, kernel='poly', degree=4)))\n            models.append(('svm5', svm.NuSVC(probability=True, nu=0.7, kernel='rbf')))\n            models.append(('svm6', svm.NuSVC(probability=True, nu=0.5, kernel='rbf')))\n            self.model = sklearn.ensemble.VotingClassifier(estimators=models, voting='soft')\n\n    def fit(self, X, y):\n        \n        \n        \n        return self.model.fit(X, y) \n        \n    def predict(self, X, probs=False):\n\n        \n        return self.model.predict_proba(X) if probs else self.model.predict(X)\n    \n    def __call__(self, X, probs=False):\n        return self.predict(X, probs)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T12:14:30.252282Z","iopub.execute_input":"2023-04-12T12:14:30.253412Z","iopub.status.idle":"2023-04-12T12:14:30.264728Z","shell.execute_reply.started":"2023-04-12T12:14:30.253358Z","shell.execute_reply":"2023-04-12T12:14:30.263536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Deep Learning\n\nAlthough we have very small amount of samples at hand, we wanted to try some deep learning solutions. Deep learning is known for its black-box nature, and its robustness generally depends on the amount of data that is fed into the model. There are some methods to counteract the lack of data at hand, which we utilize. One of the caveats (and also possibly advantages) is the immensly big solution space that it has. This is a caveat in our case, since finding the right hyperparameters takes many training iterations, many different models etc. There are some helpful API that help in solving this issue (KerasTuner from Keras, GridSearchCV from sklearn), which we have tried but in the end found to be inefficient (intuition is also a viable option). Below, more of these will be mentioned in depth.","metadata":{"papermill":{"duration":0.101099,"end_time":"2021-03-08T07:59:09.31402","exception":false,"start_time":"2021-03-08T07:59:09.212921","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 3.2.1 Model Creation\n\nFor this classifier type, we have created two models. One of them is a compact ANN, built to be used with extracted PCA features. The other is a transfer learning solution, built to be used with the faces themselves (without prior feature extraction).","metadata":{}},{"cell_type":"markdown","source":"#### 3.2.1.1 Compact DL Model\n\nBelow, the first model can be observed. There are several things to consider while building a model, and these are related with the amount of data we have as well.\n\nImportant considirations that went into building this model:\n>* Overfitting and Complexity: Because of the scarcity of data, any DL model that we will attempt to build is prone to overfitting (the phenomenon of the model losing generalization because it favors training data). Hence, we want a model that is not too simple, otherwise the model cannot be general, and test accuracy will fail. However, also because of the scarcity of data, we cannot attempt to build an extremly complex model, since this would result in the model not being able to extract patterns in our data, resulting in abysmal accuracy. Hence; it is a slippery slope, and there needs to be balance, both in terms of the number of units used and the amount of layers utilized.\n>* Regularization: Following this discussion on overfitting, we can utilize something called regularization to get in the way of overfitting, and create a more general model. There are many ways to implement regularization, with the most popular one (and the one used in this model) being the Dopout layer. This layer randomly selects _p_ percentage of the data inputted into the layer and drops them (= 0). This helps in 2 ways: the model becomes less dependent on training data, and layers capture more distinct features.\n>* Activation Function(s): Since we are doing a (multi-class) classification model, the last layer of the network has to be a softmax (although not the case here, it can also be a single unit with sigmoid if the problem at hand is a binary classification problem). The softmax function outputs probabilities for each class, and the whichever class has the maximum probability is the final prediction. For the other layers, we use an activation called Rectified Linear Unit (ReLU). ReLU is popular and practical since its gradient computation is easy, and the model can converge faster when its used. It also does not enable all neurons (if _v_ < 0, _relu(v)_ = 0), hence forward pass is also quicker. We have tried other activation functions (tanh and sigmoid), but they were sub-optimal. For future improvements, advanced activation functions such as LeakyReLU can be utilized.\n\nIn the end, we use a neural network structure that has 4 layers, with the last layer being where the classification takes place.","metadata":{}},{"cell_type":"code","source":"def create_compact_model(num_classes, input_shape):\n    \n    inputs = keras.Input(shape=input_shape)\n    x = layers.LayerNormalization(axis=-1)(inputs)\n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation=\"relu\")(x)\n    x = layers.Dropout(0.1)(x)\n    \n    if num_classes == 2:\n        activation = \"sigmoid\"\n        units = 1\n    else:\n        activation = \"softmax\"\n        units = num_classes\n    \n    outputs =layers.Dense(units, activation=activation)(x)\n    \n    \n    return {\"model\": keras.Model(inputs, outputs), \n            \"num_classes\": num_classes, \n            \"input_shape\": input_shape}","metadata":{"execution":{"iopub.status.busy":"2023-04-12T13:25:35.591174Z","iopub.execute_input":"2023-04-12T13:25:35.592331Z","iopub.status.idle":"2023-04-12T13:25:35.600871Z","shell.execute_reply.started":"2023-04-12T13:25:35.592263Z","shell.execute_reply":"2023-04-12T13:25:35.599511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.2.1.2 Transfer Learning (with an option to fine-tune) using Xception\n\nBelow, we create our second model. With such a small dataset, trying transfer learning was a no-brainer. For this model, we also added the option to fine-tune the base model. Now lets discuss the thought process that went into this model.\n\nImportant explanations for techniques that were used, and considerations that went into building this model:\n>* Transfer Learning: For some problems, similar to our case, it is more practical (and desirable, and accurate) to use an already existing model, that was trained on hefty datasets. These pre-trained models are most common in CV and NLP (famously BERT) applications. Transfer learning is one of the two approaches to using these pre-trained models. It describes freezing some or all layers of a pre-trained network, taking the top layer (which is the output layer) off and putting a smaller, untrained model on top. After that, training is done ONLY on the smaller model that has been initialized on top of the base network. This process can also be, in simplified terms, described as feature extraction. In this case, the extractor is not PCA or SIFT, but a pretrained DL model. With those features we train a smaller model, and reach an accurate model. In our case this approach was more succesful compared to fine-tuning.\n>* Fine-Tuning: Similar to transfer learning, a pre-trained model is used for fine-tuning. In the case of fine-tuning, we train the base model as well. Again we take the top layer off, and put our own smaller model (or a single empty layer), but some/all layers of the base model are also trainable. However; a yet unmentioned hyperparameters, learning rate and epoch, comes in to consideration. To find an optimal solution and to avoid overfitting, fine-tuning is done with a very small learning rate (e.g. 1e-5) and epoch (e.g. 3-7). In our case, transfer learning was more succesful, and fine-tuning overfit the data pretty quickly (or did not learn accurate general patterns).\n>* Overfitting, complexity and similar considerations to the previous model: Similar considerations went into developing the rest of the model, we use dropout to reduce chances of overfitting, use softmax for classification and we try to keep the model mildly complex so that our data's distinctions are recognized by the model.\n>* Small snippet of data augmentation: The bulk of data augmentation will be mentioned in the DL framework that we have developed, but here we do two simple steps to generalize our training data: random contrast and gaussian noise. Additionally, we mentioned dropout, and how its a regularization method to create a more general and accurate model. GaussianNoise is another layer that can be used for a similar purpose as well.\n \nWe utilize Xception as our base model. Xception is trained on the ImageNet database, and is a lighter model compared to other options (e.g. VGG, which we tried and found to be sub-optimal). The features extracted from Xception are then average pooled and inputted into a 2 layer ANN.","metadata":{}},{"cell_type":"code","source":"# TRANSFER LEARNING - Xception, VGG16\n\n\ndef create_transfer_learning_model(num_classes, input_shape, fine_tune=False, augment_data=False):\n\n    base_model = keras.applications.Xception(\n    weights='imagenet',  \n    input_shape=input_shape,\n    include_top=False)\n\n    base_model.trainable = fine_tune\n    \n    \n    inputs = keras.Input(shape=input_shape)\n\n    if augment_data:\n        \n        x = layers.RandomContrast(0.2)(inputs)\n        x = layers.GaussianNoise(0.2)(x)\n        \n    else:\n        x = inputs\n        \n    \n    x = keras.applications.xception.preprocess_input(x)\n    x = base_model(x, training=fine_tune)\n    x = layers.GlobalAveragePooling2D()(x)\n    \n    if not fine_tune:\n        x = layers.Dropout(0.1)(x)\n        x = layers.Dense(32, activation=\"relu\")(x)\n    x = layers.Dropout(0.5)(x)\n    \n    if num_classes == 2:\n        activation = \"sigmoid\"\n        units = 1\n    else:\n        activation = \"softmax\"\n        units = num_classes\n    \n    outputs =layers.Dense(units, activation=activation)(x)\n    \n    \n    return {\"model\": keras.Model(inputs, outputs), \n            \"num_classes\": num_classes, \n            \"input_shape\": input_shape}","metadata":{"execution":{"iopub.status.busy":"2023-04-12T12:07:20.236164Z","iopub.execute_input":"2023-04-12T12:07:20.236859Z","iopub.status.idle":"2023-04-12T12:07:20.247784Z","shell.execute_reply.started":"2023-04-12T12:07:20.236822Z","shell.execute_reply":"2023-04-12T12:07:20.246504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 Framework, Data Augmentation, Optimizers\n\n\n#### Data Augmentation\n\nTo counteract the scarcity of our training samples, we use data augmentation. Data augmentation is a technique to increase training data samples by different augmentation techniques, such as rotations, shifts, flips and random brightness values. This is essential to our training, and gets in the way of the model overfitting. It helps the model learn general features, independent of rotation, flips etc. For this, we use the Keras implementation of ImageDataGenerator. Using the flow funtion of said class, we obtain a data generator that returns randomly augmented training images. Below, an example of an augmented image is given. ","metadata":{}},{"cell_type":"code","source":"datagen = keras.preprocessing.image.ImageDataGenerator(\n                rotation_range=20,\n                width_shift_range=0.1,\n                height_shift_range=0.1,\n                shear_range=0.1,\n                zoom_range=0.1,\n                horizontal_flip=True,\n                brightness_range=[0.9, 1.1],\n                fill_mode='nearest',\n                dtype=\"uint8\")\n            \niterable = datagen.flow(x=train_X, batch_size=21, shuffle=False)  \n\nx = next(iterable)[11]\n\nfig, axes = plt.subplots(1,2,sharex=True,sharey=True,figsize=(8,6))\naxes[0].imshow(x, cmap=\"gray\")\naxes[0].set_title(\"Augmented Image\")\naxes[1].imshow(train_X[11], cmap=\"gray\")\naxes[1].set_title(\"Original Image\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-12T18:03:57.429890Z","iopub.execute_input":"2023-04-12T18:03:57.430984Z","iopub.status.idle":"2023-04-12T18:03:58.035529Z","shell.execute_reply.started":"2023-04-12T18:03:57.430935Z","shell.execute_reply":"2023-04-12T18:03:58.034570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nBelow, the deep learning model framework that we have developed can be observed. \n\nGeneral remarks:\n>* Categorical labels: The labels provided to us have 3 different values (0, 1, 2); however, the loss function used (and also since the output of the networks are (,3)) requires the labels to be one hot encoded. What this means is, if the label is class 2 for example, its one hot encoded represantation becomes a vector like [0, 0, 1]. There is a loss function that accepts numerical labels, but we decided to stick with categorical labels.\n>* Custom Generator: This is implemented so that the feature extractors also use augmented images. \n>* Some failures: We tried early stopping, which if it worked in a successful manner wouldve been good to utilize. However, we decided to not use it in the end since our training time is not that long (we dont have a lot of data, and our models are not that complex). So early stopping was not that useful.\n\n\n#### Optimizers\n\nWe have tried three different optimizers for this assignment, these are Stochastic Gradient Descent (SGD), Adam, and a new implementation of Adam called AdamW. Adam is the most popular choice in literature, because of its efficiency in converging to a solution. SGD is also another, simpler implementation of an optimizer, it finds a more optimal solution compared to Adam, but is slower and at times less accurate (because Adam has built in regularization parameters). AdamW is a different implementation of Adam that implements weight decay differently, and is popular in NLP applications. We wanted to try these three optimizers because of their popularity, and found that Adam gave us the best results for our problem.\n\n#### Training and Prediction\n\nFor both training and prediction we use the built in _fit()_ and _predict_ methods of the Keras model respectively. \n\n","metadata":{}},{"cell_type":"code","source":"class CustomGenerator(keras.utils.Sequence):\n    \"\"\"\n    The custom generator class, which extends the Keras Sequence framework. \n    Its purpose is to use the output of image augmentation and pass it to the \n    feature constructors that we have implemented.\n    \"\"\"\n    \n    def __init__(self, iterable, fe):\n        self.iterable = iterable\n        self.fe = fe\n        \n    def __len__(self):\n        return len(self.iterable)\n\n    def __getitem__(self, idx):\n        x, y = next(self.iterable)\n        return self.fe(x), y\n    \n\n\nclass DeepLearningModel:\n    \n    def __init__(self, model, num_classes, input_shape, feature_extractor=None):\n        \"\"\"\n        Initializations of the built model, number of classes (for One Hot Encoding) and\n        input shape (not used but just in case). If we want to apply a feature extractor, that\n        is also passed as a parameter.\n        \"\"\"\n        self.model = model\n        self.num_classes = num_classes\n        self.input_shape = input_shape\n        self.feature_extractor = feature_extractor\n    \n    def fit(self, X, y, epoch=10, lr=1e-3, batch_size=4, augment_data=False, opt=\"adam\", val_ds=None):\n        \"\"\"\n        The fit method of the framework, which:\n            - Creates categorical labels\n            - Creates a data augmentation pipeline\n            - Creates an optimizer\n            - Compiles the model (defines the loss, optimizer and accuracy metric)\n            - Fits the training data and labels, and starts the training flow\n            \n        \"\"\"\n        \n        \n        y_OHE = keras.utils.to_categorical(y, num_classes = self.num_classes)\n        \n        if augment_data:\n            \n            datagen = keras.preprocessing.image.ImageDataGenerator(\n                rotation_range=20,\n                width_shift_range=0.05,\n                height_shift_range=0.05,\n                shear_range=0.05,\n                zoom_range=0.05,\n                horizontal_flip=True,\n                brightness_range=[0.9, 1.1],\n                fill_mode='nearest',\n                dtype=\"uint8\")\n            \n            iterable = datagen.flow(x=X, y=y_OHE, batch_size=batch_size, shuffle=True)            \n            kwargs = {\"x\": iterable if self.feature_extractor is None else CustomGenerator(iterable, self.feature_extractor)}  # if feature extractor exists, create custom data flow\n            \n        else:\n            kwargs = {\"x\": X if self.feature_extractor is None else self.feature_extractor(X), \"y\": y_OHE, \"batch_size\": batch_size, \"shuffle\":True}\n    \n        \n        if opt == \"adam\":\n            optimizer=keras.optimizers.Adam(lr=lr)\n            \n        if opt == \"sgd\":\n            optimizer=keras.optimizers.SGD(lr=lr, momentum = 0.9)\n            \n        if opt == \"adamw\":\n            optimizer = keras.optimizers.experimental.AdamW(learning_rate=lr, weight_decay=5e-3)\n\n        self.model.compile( optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    \n        return self.model.fit(**kwargs, epochs=epoch, validation_data=val_ds,)\n    \n        \n    def predict(self, X, probs=False):\n        \"\"\"\n        Predicts the incoming samples, if feature extractor exists, first the features are extracted\n        from the sample and then the predictions are made.\n        \"\"\"\n        \n        if self.feature_extractor is None:\n            pred = self.model.predict(X)\n        else:\n            feats = self.feature_extractor(X)\n            pred = self.model.predict(feats)\n            \n        return pred if probs else np.argmax(pred, axis=-1)\n    \n\n    def __call__(self, X, probs=False):\n        return self.predict(X, probs)","metadata":{"papermill":{"duration":0.108542,"end_time":"2021-03-08T07:59:09.525054","exception":false,"start_time":"2021-03-08T07:59:09.416512","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T21:49:04.656395Z","iopub.execute_input":"2023-04-12T21:49:04.656947Z","iopub.status.idle":"2023-04-12T21:49:04.751645Z","shell.execute_reply.started":"2023-04-12T21:49:04.656908Z","shell.execute_reply":"2023-04-12T21:49:04.750120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Random Forest\n\nBelow is our RandomForest framework, which uses the RandomForestClassifier class of sklearn. We did not use it in our final classifiers, but we still wanted to leave it in since we tried it.","metadata":{}},{"cell_type":"code","source":"class RandomForest:\n    \n    def __init__(self, n_estimators=100, max_depth=2):\n\n        self.model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n\n    \n    def fit(self, X, y):\n        \n        return self.model.fit(X, y)\n        \n    def predict(self, X):\n\n        \n        return self.model.predict(X)\n    \n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-04-11T13:22:55.908652Z","iopub.execute_input":"2023-04-11T13:22:55.909284Z","iopub.status.idle":"2023-04-11T13:22:55.917993Z","shell.execute_reply.started":"2023-04-11T13:22:55.909234Z","shell.execute_reply":"2023-04-11T13:22:55.916611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Ensemble Model\n\nHere, we create a simple framework for a weighted ensemble model. It uses pre-trained classifier pipelines. Each pipelines predicted provabilites are obtained and a weighted sum is obtained. Afterwards the maximum class is chosen as the predicted class. ","metadata":{}},{"cell_type":"code","source":"class EnsembleModel:\n    \n    def __init__(self, trained_models, weights):\n\n        self.models = trained_models\n        self.weights = weights\n        \n    def predict(self, X):\n        \n        preds = None\n        \n        for m, w in zip(self.models, self.weights):\n            \n            if preds is None:\n                preds = w * m(X, True)\n                \n        \n            else:\n                preds += w * m(X, True)\n\n        \n        return np.argmax(preds, axis=-1)\n    \n    def __call__(self, X):\n        return self.predict(X)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T12:19:34.027049Z","iopub.execute_input":"2023-04-12T12:19:34.027457Z","iopub.status.idle":"2023-04-12T12:19:34.036655Z","shell.execute_reply.started":"2023-04-12T12:19:34.027424Z","shell.execute_reply":"2023-04-12T12:19:34.035372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Experiments\n","metadata":{"papermill":{"duration":0.102942,"end_time":"2021-03-08T07:59:09.730342","exception":false,"start_time":"2021-03-08T07:59:09.6274","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 4.1 Experiments with SVM\n\nThe experimentation with SVM is simple. We choose 20 principle components for PCA, and input the features into the model. Then we create the pipeline lambda function.","metadata":{}},{"cell_type":"code","source":"### USING SVM w/ PCA\n\nk = 20\n\naX, ay = train_X, train_y\n\nfeature_extractor = PCAFeatureExtractor(k, train_X)\nclassifier = SVM(False)\n\nclassifier.fit(feature_extractor(aX), ay)\n\nsvm_pipeline = lambda X, p=False: classifier(feature_extractor(X), p)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:23:56.030398Z","iopub.execute_input":"2023-04-12T15:23:56.031664Z","iopub.status.idle":"2023-04-12T15:23:56.391989Z","shell.execute_reply.started":"2023-04-12T15:23:56.031613Z","shell.execute_reply":"2023-04-12T15:23:56.390150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Experiements with Deep Learning Models\n\nThe experimentation for DL models are a bit more complex, with more consideration in place. Again, overfitting and generalization is one of the biggest considerations in how we choose the following hyperparameters.\n\n>* Epochs: A single epoch is a single fit of the entire dataset onto the model. A single run on the training dataset is not enough, since the model will not be able to capture patters that quickly; however, increasing this value greatly (to hundreds, thousands) is counter-intuitive. This will cause the model to specifically consider the training data, and will get in the way of generalization. We have tried values between 10-50, for transfer learning 30 epochs is enough, for the PCA model we have found 50 to be the best. \n>* Learning Rate: The learning rate is the parameter that defines the speed in which the network weights are learned. It is another tricky parameter: if its too big the model will overshoot the optimal point and oscillate between sub-optimal points, if its too small the model will not be able to reach the optimal point. Again, it has to have some balance, fast but steady. Below a simple reprasentation of how learning rate behaves can be observed in a 2D space [[1]](#1). For our purposes, values between $10^{-3}$ and $10^{-2}$ gave accurate results.\n\n <div style=\"text-align:center\"><img src=\"https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png\" width=\"40%\" height=\"40%\"></div>\n\n>* Batch Size: Batch size is another important parameter when considering generalization. A model trained with a large batch size will become a more general model. However, we do risk the model being too general (i.e. not learning patterns in training data). So again, there has to be a balanced option. In our case, 32 gave us the best results. \n\nThere are many more hyperparameters that we could've considered (most of them are optimizer specific, such as momentum (for SGD), beta1/2 and epsilon (Adam) and finally weight decay). However, these parameters have common values among literature, and putting these parameters into consideration as well would increase the complexity of our parameter space greatly. Trying different values for the previously mentioned hyperparameters were enough to find an accurate model.\n\n\n#### References\n<a id=\"1\">[1]</a> \nSetting the learning rate of your neural network. Available at: https://www.jeremyjordan.me/nn-learning-rate/","metadata":{}},{"cell_type":"markdown","source":"### 4.2.1 PCA with Deep Learning","metadata":{}},{"cell_type":"code","source":"### PCA - DEEP LEARNING\nk = 30\nfeature_extractor = PCAFeatureExtractor(k, train_X)\n# # classifier = RandomClassificationModel()\n\nkwargs = create_compact_model(num_classes=3, input_shape=k) # we do [1:] to NOT consider the batch size in the model\nprint(kwargs[\"model\"].summary())\nclassifier = DeepLearningModel(**kwargs, feature_extractor=feature_extractor)\n\n# train the model on the features\nclassifier.fit(train_X, train_y, epoch=50, lr=5e-3, batch_size=32, augment_data=True, opt=\"adam\")\n\n# # model/final pipeline\npca_dl_pipeline = lambda X, p=False: classifier(X, p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.1 Transfer Learning","metadata":{}},{"cell_type":"code","source":"### USING ONLY THE PICTURES - TRANSFER LEARNING\n\nkwargs = create_transfer_learning_model(num_classes=3, input_shape=train_X.shape[1:], augment_data=True) # we do [1:] to NOT consider the batch size in the model\nprint(kwargs[\"model\"].summary())\n\nclassifier = DeepLearningModel(**kwargs)\n\n# train the model on the features\nclassifier.fit(train_X, train_y, epoch=30, lr=2e-3, batch_size=32, augment_data=True, opt=\"adam\")\n\n# # model/final pipeline\ntf_dl_pipeline = lambda X, p=False: classifier(X, p)","metadata":{"execution":{"iopub.status.busy":"2023-04-12T20:10:46.489399Z","iopub.execute_input":"2023-04-12T20:10:46.489783Z","iopub.status.idle":"2023-04-12T20:11:15.174127Z","shell.execute_reply.started":"2023-04-12T20:10:46.489750Z","shell.execute_reply":"2023-04-12T20:11:15.173145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# em = EnsembleModel([tf_dl_pipeline, svm_pipeline], weights=[0.6, 0.4])","metadata":{"execution":{"iopub.status.busy":"2023-04-12T15:23:44.492397Z","iopub.execute_input":"2023-04-12T15:23:44.493096Z","iopub.status.idle":"2023-04-12T15:23:44.498032Z","shell.execute_reply.started":"2023-04-12T15:23:44.493058Z","shell.execute_reply":"2023-04-12T15:23:44.496972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Publishing best results","metadata":{"papermill":{"duration":0.103853,"end_time":"2021-03-08T07:59:10.903341","exception":false,"start_time":"2021-03-08T07:59:10.799488","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# evaluate performance of the model on the training set\ntrain_y_star = tf_dl_pipeline(train_X)\nprint(train_y_star)\n\nprint(f\"The performance on the training set is {accuracy_score(train_y, train_y_star):.2f}. This however, does not tell us much about the actual performance (generalisability).\")\n\n# predict the labels for the test set \ntest_y_star = tf_dl_pipeline(test_X)\n\nsubmission = test.copy().drop('img', axis = 1)\nsubmission['class'] = test_y_star\n\nsubmission\n\nsubmission.to_csv('submission.csv')","metadata":{"papermill":{"duration":0.122516,"end_time":"2021-03-08T07:59:11.356409","exception":false,"start_time":"2021-03-08T07:59:11.233893","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-12T20:12:22.166666Z","iopub.execute_input":"2023-04-12T20:12:22.167686Z","iopub.status.idle":"2023-04-12T20:12:26.978579Z","shell.execute_reply.started":"2023-04-12T20:12:22.167647Z","shell.execute_reply":"2023-04-12T20:12:26.977562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Discussion\n\nDifferences between feature extraction techniques: \n\nPCA reduces the dimensionality of the feature space by projecting the data onto a lower-dimensional space. Histogram of Oriented Gradient & Scale Invariant Feature Transform result in a high dimensional feature vector since it extracts features from different scales and orientations\nHistogram of Oriented Gradient & Scale Invariant Feature Transform features are designed to capture local patterns in an image which can be very discriminative for tasks such as object recognition and detection. PCA features are designed to capture the overall variance in the data. \nPCA can be computationally efficient, especially when the dimensionality of the input data is very high. HOG/SIFT, on the other hand, requires more computational resources due to the need to extract features at multiple scales and orientations.\n\nPCA is mainly used for data compression, where the goal is to reduce the dimensionality of high-dimensional data while preserving most of the variability in the data. \nHOG/SIFT features are particularly well-suited for object recognition tasks, where the goal is to identify an object in an image\nIn summary, the choice between PCA and HOG/SIFT depends on the specific application and the characteristics of the data being analyzed. HOG/SIFT features may work better for object recognition tasks and for data that is likely to have image transformations, while PCA may be more appropriate for data compression, tasks where interpretability is important, and situations with limited computational resources.\n\nIn terms of classifiers, lets first discuss them separetly:\n>* SVM, RandomForest, kNN: These methods, compared to DL models that we presented, are simpler. They are explainable and predictable. And for simple classification problems, they are efficient and work. However, they cannot pass a certain treshold in terms of test accuracy. For example, the best accuracy that we got from these models was SVM with PCA, and it was %74. This is good, but in theory DL models capture general patterns better because of their complexity.\n>* Compact Deep Learning: When used with PCA, this model did output good results (although perhaps with different hyperparameters it could be better).\n>* Transfer Learning: This was the best method that we were able to implement. It makes sense since it uses a pre-trained model, and we specify the last layer for our problem. For applications where small data is available, transfer learning or fine tuning become great options (along with data augmentation).\n\nIf we were to compare these models, the best one as we mentioned is Transfer Learning. Although, there is a caveat. Deep learning models, with the scarce data that we have, are not robust. For this application that is fine, but if the stakes were higher (i.e. for use in medical field, stock prediction), we would need to lean towords more robust models (such as SVM or maybe even linear models, or keep the DL model but search for more data/ methods to make the model more robust). \n\nThe biggest challenge faced while building a DL model is choosing the hyperparameters. There are so many things to consider: layer units, # of layers, learning rate, epochs, batch size... The list goes on. Even with data augmentation, you have to consider how much you want to augment the data. If its not enough, it doesnt make a difference. If its too much then the model might not be able to capture patterns in data. This is where literature, and also intuition come in. Both are needed to perfect these models.\n\nFor future improvements, several things can be considered: \n>* Variational AutoEncoders: This is something we had researched, but because of its complexity decided to leave out. It is a model that captures training data and generates new training data according to input noise. This might help in increasing the dataset at hand.\n>* Different layers: Both in terms of activation layers (LeakyRelu) and also regularizations (Instead of dropout other regularization techniques can be considered).\n>* Hyperparameter Tuning using Grid Search: To find an optimal combination of hyperparameters, API such as GridSearchCV can be utilized.\n","metadata":{"papermill":{"duration":0.116655,"end_time":"2021-03-08T07:59:11.577703","exception":false,"start_time":"2021-03-08T07:59:11.461048","status":"completed"},"tags":[]}}]}